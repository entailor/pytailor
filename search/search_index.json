{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the pyTailor documentation What is pyTailor? pyTailor is a python client for the Tailor automation and collaboration platform. See Tailor.wf for more information. With the pyTailor API you can: Turn your existing python code into parameterized and reusable workflows. Run workflows locally in your computer or distributed to dedicated worker nodes. Store your workflows as workflow definitions in the Tailor backend and share it with others. How does it work? Let's look at a motivational example. Say you have developed a set of python functions to solve some complex computing or engineering problem, and you have organized these functions into a python module called engineering_tasks.py . Say you have a three-step workflow like this: prepare input data for a set of simulations. This is handled by the function engineering_tasks.prepare_simulation_data . Let's say this function: takes a base input file as input. takes a list of parameter dicts for each simulation. produces one simulation input file for each parameter dict. run a set of simulations in parallel. A single simulation is handled by the function engineering_tasks.run_simulation . Let's say this function: takes a .inp file as input produces a .res file as output do post-processing of the simulation data. This is handled by the function engineering_tasks.post_process_simulation_data . Let's say this function: takes a set of .res files as input produces a file report.pdf returns a dict with essential post-processing results With pyTailor you can wrap these functions into PythonTasks , and then use a DAG to define how these tasks relate to each other: from pytailor import PythonTask , BranchTask , DAG , Inputs , Outputs , Files import engineering_tasks inputs = Inputs () outputs = Outputs () files = Files () with DAG ( name = \"Advanced simulation dag\" ) as dag : t1 = PythonTask ( name = \"Pre-processing\" , function = engineering_tasks . prepare_simulation_data , kwargs = { \"parameters\" : inputs . pre_proc_data , \"base_file\" : files . base_file }, download = files . base_file , upload = { files . inp_file : \"sim_inp_file_*.inp\" } ) with BranchTask ( name = \"Parallel simulations\" , branch_files = files . inp_file , parents = t1 ) as branch : PythonTask ( name = \"Simulation\" , function = engineering_tasks . run_simulation , args = [ files . inp_file [ 0 ]], download = files . inp_file , upload = { files . res_file : \"*.res\" } ) PythonTask ( name = \"Post-processing\" , function = engineering_tasks . post_process_simulation_data , args = [ files . res_file ], download = files . res_file , upload = { files . report : \"report.pdf\" }, output_to = outputs . essential_results , parents = branch ) The DAG object represents the recipe for how the the computations shall be performed. By instantiating a DAG no computations are performed, note that we are just referencing the functions we want to use, we are not calling them. DAG is short for Directed Asyclic Graph , a term used to describe the logical flow of computations in a workflow. The DAG defined above is visualized below: A key feature in this DAG is the use of BranchTask to achieve parallelization or \"fan-out\" functionality. The term branching is used to describe this functionality, where one branch is created for each simulation. The Inputs , Outputs and Files objects are helper-objects for parameterization . When we e.g. say kwargs={\"parameters\": inputs.pre_proc_data} we are specifying that the value for the \"parameters\" keyword argument is parameterized and shall be looked up from the pre_proc_data name in the workflow's inputs when the task is executed. The concept of parameterization becomes clearer when we see how inputs , outputs and files are defined when we run a Workflow below. We now have a parameterized DAG describing the recipe of how we want to perform our computing workflow. Based on this definition we can run a Workflow , and we have sereral options: run it directly run it distributed (i.e in parallel, and optionally on several worker machines) Store it as a WorkflowDefinition so that it can be executed directly from the Tailor Webapp. For this example we're just going to run the workflow directly: from pytailor import Project , FileSet , Workflow # open a project prj = Project . from_name ( \"Test\" ) # define inputs workflow_inputs = { \"pre_proc_data\" : [ { \"param1\" : 0 }, { \"param1\" : 1 }, { \"param1\" : 2 }, { \"param1\" : 3 }, { \"param1\" : 4 }, ] } # create a fileset and upload input files fileset = FileSet ( prj ) fileset . upload ( base_file = [ \"testfiles/testfile.inp\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"simulation workflow\" , inputs = workflow_inputs , fileset = fileset ) # run the workflow wf . run () Here we have introduced three new classes from the pyTailor API: Project . A Tailor workflow has to be run in the context of a project. FileSet . Represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run. Workflow . Represents a single workflow run . In order to run a workflow we first instantiate the Workflow object, and then we call the Workflow.run method, which will start executing the workflow in the current python process. Note The direct mode of execution used here is handy when developing new workflows and for testing and debugging of new DAGs. For production workflow runs, distributed mode is suitable. See the worker tutorial for more information. Once the workflow has been started it can be monitored from the Tailor webapp. Below is shown how the workflow can be found in the list of workflows for the specific project by searching for the workflow name. When the worklfow is selected the workflow files appear on the right side for inspection and direct download. Bu clicking on the Details link the workflow can be further inspected in the details view. Get started with pyTailor Head over to the Getting started section for instructions on how to setup pyTailor. Once you are setup and are able to run the basic examples you can start working through the tutorials . At any time you can visit the Concepts section to read about the core concepts of Tailor. You can also consult the API Reference for documentation of the pyTailor API.","title":"Home"},{"location":"#welcome-to-the-pytailor-documentation","text":"","title":"Welcome to the pyTailor documentation"},{"location":"#what-is-pytailor","text":"pyTailor is a python client for the Tailor automation and collaboration platform. See Tailor.wf for more information. With the pyTailor API you can: Turn your existing python code into parameterized and reusable workflows. Run workflows locally in your computer or distributed to dedicated worker nodes. Store your workflows as workflow definitions in the Tailor backend and share it with others.","title":"What is pyTailor?"},{"location":"#how-does-it-work","text":"Let's look at a motivational example. Say you have developed a set of python functions to solve some complex computing or engineering problem, and you have organized these functions into a python module called engineering_tasks.py . Say you have a three-step workflow like this: prepare input data for a set of simulations. This is handled by the function engineering_tasks.prepare_simulation_data . Let's say this function: takes a base input file as input. takes a list of parameter dicts for each simulation. produces one simulation input file for each parameter dict. run a set of simulations in parallel. A single simulation is handled by the function engineering_tasks.run_simulation . Let's say this function: takes a .inp file as input produces a .res file as output do post-processing of the simulation data. This is handled by the function engineering_tasks.post_process_simulation_data . Let's say this function: takes a set of .res files as input produces a file report.pdf returns a dict with essential post-processing results With pyTailor you can wrap these functions into PythonTasks , and then use a DAG to define how these tasks relate to each other: from pytailor import PythonTask , BranchTask , DAG , Inputs , Outputs , Files import engineering_tasks inputs = Inputs () outputs = Outputs () files = Files () with DAG ( name = \"Advanced simulation dag\" ) as dag : t1 = PythonTask ( name = \"Pre-processing\" , function = engineering_tasks . prepare_simulation_data , kwargs = { \"parameters\" : inputs . pre_proc_data , \"base_file\" : files . base_file }, download = files . base_file , upload = { files . inp_file : \"sim_inp_file_*.inp\" } ) with BranchTask ( name = \"Parallel simulations\" , branch_files = files . inp_file , parents = t1 ) as branch : PythonTask ( name = \"Simulation\" , function = engineering_tasks . run_simulation , args = [ files . inp_file [ 0 ]], download = files . inp_file , upload = { files . res_file : \"*.res\" } ) PythonTask ( name = \"Post-processing\" , function = engineering_tasks . post_process_simulation_data , args = [ files . res_file ], download = files . res_file , upload = { files . report : \"report.pdf\" }, output_to = outputs . essential_results , parents = branch ) The DAG object represents the recipe for how the the computations shall be performed. By instantiating a DAG no computations are performed, note that we are just referencing the functions we want to use, we are not calling them. DAG is short for Directed Asyclic Graph , a term used to describe the logical flow of computations in a workflow. The DAG defined above is visualized below: A key feature in this DAG is the use of BranchTask to achieve parallelization or \"fan-out\" functionality. The term branching is used to describe this functionality, where one branch is created for each simulation. The Inputs , Outputs and Files objects are helper-objects for parameterization . When we e.g. say kwargs={\"parameters\": inputs.pre_proc_data} we are specifying that the value for the \"parameters\" keyword argument is parameterized and shall be looked up from the pre_proc_data name in the workflow's inputs when the task is executed. The concept of parameterization becomes clearer when we see how inputs , outputs and files are defined when we run a Workflow below. We now have a parameterized DAG describing the recipe of how we want to perform our computing workflow. Based on this definition we can run a Workflow , and we have sereral options: run it directly run it distributed (i.e in parallel, and optionally on several worker machines) Store it as a WorkflowDefinition so that it can be executed directly from the Tailor Webapp. For this example we're just going to run the workflow directly: from pytailor import Project , FileSet , Workflow # open a project prj = Project . from_name ( \"Test\" ) # define inputs workflow_inputs = { \"pre_proc_data\" : [ { \"param1\" : 0 }, { \"param1\" : 1 }, { \"param1\" : 2 }, { \"param1\" : 3 }, { \"param1\" : 4 }, ] } # create a fileset and upload input files fileset = FileSet ( prj ) fileset . upload ( base_file = [ \"testfiles/testfile.inp\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"simulation workflow\" , inputs = workflow_inputs , fileset = fileset ) # run the workflow wf . run () Here we have introduced three new classes from the pyTailor API: Project . A Tailor workflow has to be run in the context of a project. FileSet . Represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run. Workflow . Represents a single workflow run . In order to run a workflow we first instantiate the Workflow object, and then we call the Workflow.run method, which will start executing the workflow in the current python process. Note The direct mode of execution used here is handy when developing new workflows and for testing and debugging of new DAGs. For production workflow runs, distributed mode is suitable. See the worker tutorial for more information. Once the workflow has been started it can be monitored from the Tailor webapp. Below is shown how the workflow can be found in the list of workflows for the specific project by searching for the workflow name. When the worklfow is selected the workflow files appear on the right side for inspection and direct download. Bu clicking on the Details link the workflow can be further inspected in the details view.","title":"How does it work?"},{"location":"#get-started-with-pytailor","text":"Head over to the Getting started section for instructions on how to setup pyTailor. Once you are setup and are able to run the basic examples you can start working through the tutorials . At any time you can visit the Concepts section to read about the core concepts of Tailor. You can also consult the API Reference for documentation of the pyTailor API.","title":"Get started with pyTailor"},{"location":"api/parameterization/","text":"Parameterization Classes Inputs class pytailor. Inputs ( ) Helper object for inputs parameterization. Basic usage from pytailor import PythonTask , DAG , Inputs , Workflow , Project inputs = Inputs () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = inputs . hello , ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , inputs = { \"hello\" : [ \"Hello, world!\" ]} ) wf . run () Outputs class pytailor. Outputs ( ) Helper object for outputs parameterization. Basic usage from pytailor import PythonTask , DAG , Outputs , Workflow , Project outputs = Outputs () with DAG () as dag : t1 = PythonTask ( function = 'os.getcwd' , output_to = outputs . curdir ) PythonTask ( function = 'builtins.print' , args = [ outputs . curdir ], ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , ) wf . run () print ( wf . outputs ) Files class pytailor. Files ( ) Helper object for files parameterization. Basic usage from pytailor import PythonTask , DAG , Files , Workflow , Project , FileSet files = Files () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = [ \"This tasks download the file:\" , files . inpfile [ 0 ]], download = files . inpfile ) prj = Project . from_name ( \"Test\" ) fileset = FileSet ( project = prj ) fileset . upload ( inpfile = [ \"my_file.txt\" ]) wf = Workflow ( dag = dag , project = prj , fileset = fileset ) wf . run ()","title":"Parameterization classes"},{"location":"api/parameterization/#parameterization-classes","text":"","title":"Parameterization Classes"},{"location":"api/parameterization/#inputs","text":"class pytailor. Inputs ( ) Helper object for inputs parameterization. Basic usage from pytailor import PythonTask , DAG , Inputs , Workflow , Project inputs = Inputs () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = inputs . hello , ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , inputs = { \"hello\" : [ \"Hello, world!\" ]} ) wf . run ()","title":"Inputs"},{"location":"api/parameterization/#outputs","text":"class pytailor. Outputs ( ) Helper object for outputs parameterization. Basic usage from pytailor import PythonTask , DAG , Outputs , Workflow , Project outputs = Outputs () with DAG () as dag : t1 = PythonTask ( function = 'os.getcwd' , output_to = outputs . curdir ) PythonTask ( function = 'builtins.print' , args = [ outputs . curdir ], ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , ) wf . run () print ( wf . outputs )","title":"Outputs"},{"location":"api/parameterization/#files","text":"class pytailor. Files ( ) Helper object for files parameterization. Basic usage from pytailor import PythonTask , DAG , Files , Workflow , Project , FileSet files = Files () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = [ \"This tasks download the file:\" , files . inpfile [ 0 ]], download = files . inpfile ) prj = Project . from_name ( \"Test\" ) fileset = FileSet ( project = prj ) fileset . upload ( inpfile = [ \"my_file.txt\" ]) wf = Workflow ( dag = dag , project = prj , fileset = fileset ) wf . run ()","title":"Files"},{"location":"api/schema/","text":"Schema Definition Classes InputsSchema class pytailor. InputsSchema ( inputs ) Generator for inputsschema to define workflow definition Basic usage Following example define a jsonschema, that sets \"print\" as a required property for inputs and that the value must be a string example_inputs = { 'print' : 'Hello, world!' } inputsschema = InputsSchema ( inputs = example_inputs ) Add 'Hello, world' as a default for property 'print' example_inputs = { 'print' : 'Hello, world!' } inputsschema . add_defaults ( example_inputs ) # Set 'Hello, world' and 'Hello, tailor!' as only allowed alternatives for property 'print': enum_inputs = { 'print' : [ 'Hello, world!' , 'Hello, tailor!' ]} inputsschema . add_enums ( enum_inputs ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_defaults ( self , default_inputs ) Parameters default_inputs (dict) S Specify an example input schema that is valid for your workflow definition with defaults. add_enums ( self , enum_inputs ) Parameters enum_inputs (dict) S Specify altenatives for your inputsschema with a input schema where the property's values in a list represents the alternatives to_dict ( self ) Serialize input schema to_json ( self , filename , indent=4 ) a json file FilesSchema class pytailor. FilesSchema ( tags=None , exts=None , multiples=None , requireds=None , titles=None , descriptions=None ) Generator for filesschema to define workflow definition Basic usage files_ex1 = { \"tag\" : \"my_tag\" , \"ext\" : [ \"txt\" ], } filesschema = FilesSchema () filesschema . add_file ( tag = \"my_tag\" , ext = [ \"txt\" ]) files_ex2 = { \"tag\" : \"my__new_tag\" , \"ext\" : [ \"txt\" ], \"title\" : \"Coordinates\" , \"multiple\" : True , \"description\" : \"A file with coordinate values of nodes\" , \"required\" : True , } filesschema = FilesSchema () filesschema . add_file ( ** files_ex2 ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_file ( self , tag , ext=None , multiple=False , required=True , title=None , description='' ) Parameters tag (str) Specify tag for file. tag (str) Specify allowed extension for file mutiple (bool) Specify whether multiples files are allowed required (bool) Specify whether file(s) are required title (str) Specify the title to be shown in GUI description (str) Specify the description to be shown in GUI to_dict ( self ) Serialize files schema to_json ( self , filename , indent=4 ) Description class pytailor. Description ( name=None , description_string=None ) Workflow definition description generator to define workflow definition Basic usage wf_def_description = \"This example explains branchtasks\" wf_def_name = \"branch task example\" from pytailor import PythonTask , BranchTask , DAG with DAG ( name = \"duplicate dag example\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"* / .txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) description = Description . from_dag ( dag , wf_def_name = wf_def_name , wf_def_description = wf_def_description ) Parameters name (dict) Specify a name for the workflow definition description_string (str) Specify a description string in markdown format from_dag ( dag , wf_def_name='' , wf_def_description='' ) Generates a workflow definition description from a pytailor DAG class Parameters dag (DAG) Specify your DAG for workflow definition wf_def_name (str) Specify a name for workflow definition wf_def_description (str) Specify an overall description, what are the main objectives of your workflow definition? What are the main steps in the DAG? to_string ( self ) to_markdown ( self , filename='Readme.MD' )","title":"Schema definition classes"},{"location":"api/schema/#schema-definition-classes","text":"","title":"Schema Definition Classes"},{"location":"api/schema/#inputsschema","text":"class pytailor. InputsSchema ( inputs ) Generator for inputsschema to define workflow definition Basic usage Following example define a jsonschema, that sets \"print\" as a required property for inputs and that the value must be a string example_inputs = { 'print' : 'Hello, world!' } inputsschema = InputsSchema ( inputs = example_inputs ) Add 'Hello, world' as a default for property 'print' example_inputs = { 'print' : 'Hello, world!' } inputsschema . add_defaults ( example_inputs ) # Set 'Hello, world' and 'Hello, tailor!' as only allowed alternatives for property 'print': enum_inputs = { 'print' : [ 'Hello, world!' , 'Hello, tailor!' ]} inputsschema . add_enums ( enum_inputs ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_defaults ( self , default_inputs ) Parameters default_inputs (dict) S Specify an example input schema that is valid for your workflow definition with defaults. add_enums ( self , enum_inputs ) Parameters enum_inputs (dict) S Specify altenatives for your inputsschema with a input schema where the property's values in a list represents the alternatives to_dict ( self ) Serialize input schema to_json ( self , filename , indent=4 ) a json file","title":"InputsSchema"},{"location":"api/schema/#filesschema","text":"class pytailor. FilesSchema ( tags=None , exts=None , multiples=None , requireds=None , titles=None , descriptions=None ) Generator for filesschema to define workflow definition Basic usage files_ex1 = { \"tag\" : \"my_tag\" , \"ext\" : [ \"txt\" ], } filesschema = FilesSchema () filesschema . add_file ( tag = \"my_tag\" , ext = [ \"txt\" ]) files_ex2 = { \"tag\" : \"my__new_tag\" , \"ext\" : [ \"txt\" ], \"title\" : \"Coordinates\" , \"multiple\" : True , \"description\" : \"A file with coordinate values of nodes\" , \"required\" : True , } filesschema = FilesSchema () filesschema . add_file ( ** files_ex2 ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_file ( self , tag , ext=None , multiple=False , required=True , title=None , description='' ) Parameters tag (str) Specify tag for file. tag (str) Specify allowed extension for file mutiple (bool) Specify whether multiples files are allowed required (bool) Specify whether file(s) are required title (str) Specify the title to be shown in GUI description (str) Specify the description to be shown in GUI to_dict ( self ) Serialize files schema to_json ( self , filename , indent=4 )","title":"FilesSchema"},{"location":"api/schema/#description","text":"class pytailor. Description ( name=None , description_string=None ) Workflow definition description generator to define workflow definition Basic usage wf_def_description = \"This example explains branchtasks\" wf_def_name = \"branch task example\" from pytailor import PythonTask , BranchTask , DAG with DAG ( name = \"duplicate dag example\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"* / .txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) description = Description . from_dag ( dag , wf_def_name = wf_def_name , wf_def_description = wf_def_description ) Parameters name (dict) Specify a name for the workflow definition description_string (str) Specify a description string in markdown format from_dag ( dag , wf_def_name='' , wf_def_description='' ) Generates a workflow definition description from a pytailor DAG class Parameters dag (DAG) Specify your DAG for workflow definition wf_def_name (str) Specify a name for workflow definition wf_def_description (str) Specify an overall description, what are the main objectives of your workflow definition? What are the main steps in the DAG? to_string ( self ) to_markdown ( self , filename='Readme.MD' )","title":"Description"},{"location":"api/taskdefs/","text":"Task definition classes PythonTask class pytailor. PythonTask ( function , name=None , parents=None , owner=None , download=None , upload=None , args=None , kwargs=None , output_to=None , output_extraction=None , use_storage_dirs=True ) Task for running python code. Basic usage pytask = PythonTask ( function = 'builtins.print' , args = 'Hello, world!' , name = 'My first task' ) Parameters function (str or Callable) Python callable. Must be importable in the executing python env. name (str, optional) A default name is used if not provided. parents (BaseTask or List[BaseTask], optional) Specify one or more upstream tasks that this task depends on. download (str, list or Parameterization, optional) Provide one or more file tags. These file tags refer to files in the storage object associated with the workflow run. upload (dict, optional) Specify files to send back to the storage object after a task has been run. Dict format is {tag1: val1, tag2: val2, ...} where val can be: one or more query expressions(str og list) which is applied to the return value from callable . File names resulting from the query are then uploaded to storage under the given tag. one or more glob-style strings (str og list) which is applied in the task working dir. matching files are uploaded under the given tag. args (list, str or Parameterization, optional) Arguments to be passed as positional arguments to function . Accepts a list of ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a list. See the examples for how parameterization objects and query expressions are used. kwargs (dict, str or Parameterization, optional) Arguments to be passed as keyword arguments to function . accepts a kwargs dict where values can be ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a dict. See the examples for how parameterization objects and query expressions are used. output_to (str or Parameterization, optional) The return value of the callable is stored under the provided name in the workflow outputs . This value is then available for downstream task. output_extraction (dict, optional) Provide a dict of name: expr where expr are query-expressions to extract parts of the return value of the callable. The keys of the dict are used as names for storing in the workflow outputs which becomes available for downstream tasks. to_dict ( self ) Serialize task definition. from_dict ( d ) Create from serialized task definition. DAG class pytailor. DAG ( tasks=None , name=None , parents=None , owner=None , links=None ) Represents a Directed Acyclic Graph, i.e. a DAG. Parameters tasks : BaseTask or List[BaseTask] Python, Duplicate or WorkflowSpec objects. name : str, optional A default name is used if not provided. parents : BaseTask or List[BaseTask], optional Specify one or more upstream tasks that this task depends on. links : dict, optional Parent/children relationships can be specified with the dict on the form {parent_def: [child_def1, child_def2], ...}. Definition references may either be indices (ints) into tasks or BaseTask instances. Note that links may also be defined on task objects with the parents argument instead of using links: (parents=[parent_def1, parent_def2]) to_dict ( self ) from_dict ( d ) BranchTask class pytailor. BranchTask ( task=None , name=None , parents=None , owner=None , branch_data=None , branch_files=None ) Dynamically branch a task or DAG during workflow execution. BranchTask Provides parallelization or \"fan-out\" functionality. The task is duplicated based on branch_data or branch_files . At least one of these must be specified. Parameters task : BaseTask Task to be duplicated (PythonTask, BranchTask or DAG). name : str, optional A default name is used if not provided. parents : BaseTask or List[BaseTask], optional Specify one or more upstream tasks that this task depends on. branch_data : list or str, optional Data to be used as basis for branching. Accepts a query-expression or a list of query-expressions. The queries must evaluate to a list or a dict. If the query evaluates to a dict, that dict must have integer keys to represent the index of each branch. branch_files : list or str, optional Files to be used as basis for branching. Accepts a file tag or a list of file tags. to_dict ( self ) from_dict ( d )","title":"Task definition classes"},{"location":"api/taskdefs/#task-definition-classes","text":"","title":"Task definition classes"},{"location":"api/taskdefs/#pythontask","text":"class pytailor. PythonTask ( function , name=None , parents=None , owner=None , download=None , upload=None , args=None , kwargs=None , output_to=None , output_extraction=None , use_storage_dirs=True ) Task for running python code. Basic usage pytask = PythonTask ( function = 'builtins.print' , args = 'Hello, world!' , name = 'My first task' ) Parameters function (str or Callable) Python callable. Must be importable in the executing python env. name (str, optional) A default name is used if not provided. parents (BaseTask or List[BaseTask], optional) Specify one or more upstream tasks that this task depends on. download (str, list or Parameterization, optional) Provide one or more file tags. These file tags refer to files in the storage object associated with the workflow run. upload (dict, optional) Specify files to send back to the storage object after a task has been run. Dict format is {tag1: val1, tag2: val2, ...} where val can be: one or more query expressions(str og list) which is applied to the return value from callable . File names resulting from the query are then uploaded to storage under the given tag. one or more glob-style strings (str og list) which is applied in the task working dir. matching files are uploaded under the given tag. args (list, str or Parameterization, optional) Arguments to be passed as positional arguments to function . Accepts a list of ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a list. See the examples for how parameterization objects and query expressions are used. kwargs (dict, str or Parameterization, optional) Arguments to be passed as keyword arguments to function . accepts a kwargs dict where values can be ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a dict. See the examples for how parameterization objects and query expressions are used. output_to (str or Parameterization, optional) The return value of the callable is stored under the provided name in the workflow outputs . This value is then available for downstream task. output_extraction (dict, optional) Provide a dict of name: expr where expr are query-expressions to extract parts of the return value of the callable. The keys of the dict are used as names for storing in the workflow outputs which becomes available for downstream tasks. to_dict ( self ) Serialize task definition. from_dict ( d ) Create from serialized task definition.","title":"PythonTask"},{"location":"api/taskdefs/#dag","text":"class pytailor. DAG ( tasks=None , name=None , parents=None , owner=None , links=None ) Represents a Directed Acyclic Graph, i.e. a DAG.","title":"DAG"},{"location":"api/taskdefs/#branchtask","text":"class pytailor. BranchTask ( task=None , name=None , parents=None , owner=None , branch_data=None , branch_files=None ) Dynamically branch a task or DAG during workflow execution. BranchTask Provides parallelization or \"fan-out\" functionality. The task is duplicated based on branch_data or branch_files . At least one of these must be specified.","title":"BranchTask"},{"location":"documentation/concepts/","text":"Tailor concepts DAGs DAGs are used to define relationships between tasks in a Workflow in the form of a Directed Acyclic Graph. Task definitions Task definitions are parameterized and reusable blueprints for computing tasks. In Pytailor, tasks definitions are created using the task definition classes The available task definition classes are: PythonTask This is the basic building block. Used to define the execution of a single Python function (callable). BranchTask ... By combining the different task types, arbitrary complex tasks can be defined. A non-trivial task definition will typically consist of a DAG at the top-level, which in turn consist of other task definitions as illustrated in ... Note Tailor is still in development and more task definition classes are likely to be introduced in the future to extend functionality. Workflows Workflows are instantiated DAGs with a given set of inputs and files. Workflows are stored on the Tailor backend under a given Project . dag project inputs (parameters, JSON) files (inputs files) worker requirements In Pytailor, workflows are represented by the Workflow class Tasks Tasks are instantiated task definitions . Belongs to a Workflow . Workflow definitions Workflow definitions are used to store a DAG along with requirements (schema) for inputs and files. Workflow definitions are stored in the backend and can be made available for other users. Workflow subscriptions Workflow definitions can be shared between users with Workflow subscriptions . Tailor Accounts Tailor Projects","title":"Tailor concepts"},{"location":"documentation/concepts/#tailor-concepts","text":"","title":"Tailor concepts"},{"location":"documentation/concepts/#dags","text":"DAGs are used to define relationships between tasks in a Workflow in the form of a Directed Acyclic Graph.","title":"DAGs"},{"location":"documentation/concepts/#task-definitions","text":"Task definitions are parameterized and reusable blueprints for computing tasks. In Pytailor, tasks definitions are created using the task definition classes The available task definition classes are: PythonTask This is the basic building block. Used to define the execution of a single Python function (callable). BranchTask ... By combining the different task types, arbitrary complex tasks can be defined. A non-trivial task definition will typically consist of a DAG at the top-level, which in turn consist of other task definitions as illustrated in ... Note Tailor is still in development and more task definition classes are likely to be introduced in the future to extend functionality.","title":"Task definitions"},{"location":"documentation/concepts/#workflows","text":"Workflows are instantiated DAGs with a given set of inputs and files. Workflows are stored on the Tailor backend under a given Project . dag project inputs (parameters, JSON) files (inputs files) worker requirements In Pytailor, workflows are represented by the Workflow class","title":"Workflows"},{"location":"documentation/concepts/#tasks","text":"Tasks are instantiated task definitions . Belongs to a Workflow .","title":"Tasks"},{"location":"documentation/concepts/#workflow-definitions","text":"Workflow definitions are used to store a DAG along with requirements (schema) for inputs and files. Workflow definitions are stored in the backend and can be made available for other users.","title":"Workflow definitions"},{"location":"documentation/concepts/#workflow-subscriptions","text":"Workflow definitions can be shared between users with Workflow subscriptions .","title":"Workflow subscriptions"},{"location":"documentation/concepts/#tailor-accounts","text":"","title":"Tailor Accounts"},{"location":"documentation/concepts/#tailor-projects","text":"","title":"Tailor Projects"},{"location":"documentation/contexts/","text":"Contexts Contexts represents the data and files associated with a workflow. The context consist of three datastructures: inputs outputs files inputs are provided by the user during workflow specification, e.g. from tailor import PythonTask , WorkflowSpec # a task definition t1 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) t2 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) inputs = { input_number : - 123 } wf_spec = WorkflowSpec ( ) Note The inputs and outputs datastructures must be JSON-serializable, which limits the data types which can be used. In the future more sophisticated serialization may be applied to allow other object types, e.g. numpy arrays. For data that is not JSON-compatible you can serialize the data to file and use the file-piping mechanisms to send the data to your tasks. Context queries Contexts can be queried using the YAQL query language. Context queries can be used in task definitions as a means to parameterize inputs. When jobs are executed, the queries are performed on the context associated with the current workflow run. Context queries, when used in task definitions, are specified using a special syntax: '<% query-expression %>' . This syntax tells tailor to Scoped contexts Scoped contexts arise when tasks are duplicated. Consider the following DAG, consisting of two PythonTasks: +---+ | t1 | PythonTask ( ... , output_to = 'out' ) +-+-+ | | +- v -+ | t2 | PythonTask ( ... , args = '<% $.outputs.out %>' ) +---+ When this DAG is duplicated with a BranchTask","title":"Contexts"},{"location":"documentation/contexts/#contexts","text":"Contexts represents the data and files associated with a workflow. The context consist of three datastructures: inputs outputs files inputs are provided by the user during workflow specification, e.g. from tailor import PythonTask , WorkflowSpec # a task definition t1 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) t2 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) inputs = { input_number : - 123 } wf_spec = WorkflowSpec ( ) Note The inputs and outputs datastructures must be JSON-serializable, which limits the data types which can be used. In the future more sophisticated serialization may be applied to allow other object types, e.g. numpy arrays. For data that is not JSON-compatible you can serialize the data to file and use the file-piping mechanisms to send the data to your tasks.","title":"Contexts"},{"location":"documentation/contexts/#context-queries","text":"Contexts can be queried using the YAQL query language. Context queries can be used in task definitions as a means to parameterize inputs. When jobs are executed, the queries are performed on the context associated with the current workflow run. Context queries, when used in task definitions, are specified using a special syntax: '<% query-expression %>' . This syntax tells tailor to","title":"Context queries"},{"location":"documentation/contexts/#scoped-contexts","text":"Scoped contexts arise when tasks are duplicated. Consider the following DAG, consisting of two PythonTasks: +---+ | t1 | PythonTask ( ... , output_to = 'out' ) +-+-+ | | +- v -+ | t2 | PythonTask ( ... , args = '<% $.outputs.out %>' ) +---+ When this DAG is duplicated with a BranchTask","title":"Scoped contexts"},{"location":"documentation/getting_started/","text":"Getting started To use Pytailor you need to connect to a Tailor backend. The easiest way to get started with Tailor is to sign up for a free account at Tailor.wf . For other options please contact us . In the following it us assumed that you are using Tailor.wf as your backend service. Installation First time setup 1. Install pytailor Official releases of pytailor are available on pypi and can easily be installed with pip: pip install pytailor Note You can also clone the repository on github and install pytailor with poetry : git clone https://github.com/entailor/pytailor.git Then, from the project root run: poetry install To install without development dependencies: poetry install --no-dev 2. Configure backend When pytailor is installed your ca use the CLI to set up a barebone config file: tailor init This command generates a config file .tailor/config.toml under your home directory with the following content: API_KEY = < API KEY GOES HERE > API_BASE_URL = < URL FOR THE BACKEND REST API GOES HERE > Note It is also possible to configure pytailor with environmental variables by prefixing the environmental variables with PYTAILOR_ . E.g. to set the API, key put it in an environmental variable called PYTAILOR_API_KEY . Update Pytailor Basic usage With Pytailor installed and a backend properly configured you should be able to run the following example: from tailor import PythonTask , DAG In this example, several key concepts are illustrated: ... walk through code... Please consult the Consepts page...","title":"Getting started"},{"location":"documentation/getting_started/#getting-started","text":"To use Pytailor you need to connect to a Tailor backend. The easiest way to get started with Tailor is to sign up for a free account at Tailor.wf . For other options please contact us . In the following it us assumed that you are using Tailor.wf as your backend service.","title":"Getting started"},{"location":"documentation/getting_started/#installation","text":"","title":"Installation"},{"location":"documentation/getting_started/#first-time-setup","text":"","title":"First time setup"},{"location":"documentation/getting_started/#1-install-pytailor","text":"Official releases of pytailor are available on pypi and can easily be installed with pip: pip install pytailor Note You can also clone the repository on github and install pytailor with poetry : git clone https://github.com/entailor/pytailor.git Then, from the project root run: poetry install To install without development dependencies: poetry install --no-dev","title":"1. Install pytailor"},{"location":"documentation/getting_started/#2-configure-backend","text":"When pytailor is installed your ca use the CLI to set up a barebone config file: tailor init This command generates a config file .tailor/config.toml under your home directory with the following content: API_KEY = < API KEY GOES HERE > API_BASE_URL = < URL FOR THE BACKEND REST API GOES HERE > Note It is also possible to configure pytailor with environmental variables by prefixing the environmental variables with PYTAILOR_ . E.g. to set the API, key put it in an environmental variable called PYTAILOR_API_KEY .","title":"2. Configure backend"},{"location":"documentation/getting_started/#update-pytailor","text":"","title":"Update Pytailor"},{"location":"documentation/getting_started/#basic-usage","text":"With Pytailor installed and a backend properly configured you should be able to run the following example: from tailor import PythonTask , DAG In this example, several key concepts are illustrated: ... walk through code... Please consult the Consepts page...","title":"Basic usage"}]}