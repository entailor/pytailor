{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the pyTailor documentation What is pytailor? Pytailor is a python client for the Tailor automation and collaboration platform. See tailor.wf for more information. With the pytailor API you can: Turn your existing python code into parameterized and reusable workflows. Run workflows locally in your computer or distributed to dedicated worker nodes. Store your workflows as workflow definitions in the Tailor backend and share it with others. How does it work? Let's look at a motivational example. Say you have developed a set of python functions to solve some complex computing or engineering problem, and you have organized these functions into a python module called engineering_tasks.py . Say you have a three-step workflow like this: prepare input data for a set of simulations. This is handled by the function engineering_tasks.prepare_simulation_data . Let's say this function: takes a base input file as input. takes a list of parameter dicts for each simulation. produces one simulation input file for each parameter dict. run a set of simulations in parallel. A single simulation is handled by the function engineering_tasks.run_simulation . Let's say this function: takes a .inp file as input produces a .res file as output do post-processing of the simulation data. This is handled by the function engineering_tasks.post_process_simulation_data . Let's say this function: takes a set of .res files as input produces a file report.pdf returns a dict with essential post-processing results With pytailor you can wrap these functions into PythonTasks , and then use a DAG to define how these tasks relate to each other: from pytailor import PythonTask , BranchTask , DAG , Inputs , Outputs , Files import engineering_tasks inputs = Inputs () outputs = Outputs () files = Files () with DAG ( name = \"Advanced simulation dag\" ) as dag : t1 = PythonTask ( name = \"Pre-processing\" , function = engineering_tasks . prepare_simulation_data , kwargs = { \"parameters\" : inputs . pre_proc_data , \"base_file\" : files . base_file }, download = files . base_file , upload = { files . inp_file : \"sim_inp_file_*.inp\" } ) with BranchTask ( name = \"Parallel simulations\" , branch_files = files . inp_file , parents = t1 ) as branch : PythonTask ( name = \"Simulation\" , function = engineering_tasks . run_simulation , args = [ files . inp_file [ 0 ]], download = files . inp_file , upload = { files . res_file : \"*.res\" } ) PythonTask ( name = \"Post-processing\" , function = engineering_tasks . post_process_simulation_data , args = [ files . res_file ], download = files . res_file , upload = { files . report : \"report.pdf\" }, output_to = outputs . essential_results , parents = branch ) The DAG object represents the recipe for how the the computations shall be performed. By instantiating a DAG no computations are performed, note that we are just referencing the functions we want to use, we are not calling them. DAG is short for Directed Asyclic Graph , a term used to describe the logical flow of computations in a workflow. The DAG defined above is visualized below: A key feature in this DAG is the use of BranchTask to achieve parallelization or \"fan-out\" functionality. The term branching is used to describe this functionality, where one branch is created for each simulation. The Inputs , Outputs and Files objects are helper-objects for parameterization . When we e.g. say kwargs={\"parameters\": inputs.pre_proc_data} we are specifying that the value for the \"parameters\" keyword argument is parameterized and shall be looked up from the pre_proc_data name in the workflow's inputs when the task is executed. The concept of parameterization becomes clearer when we see how inputs , outputs and files are defined when we run a Workflow below. We now have a parameterized DAG describing the recipe of how we want to perform our computing workflow. Based on this definition we can run a Workflow , and we have several options: run it directly run it distributed (i.e in parallel, and optionally on several worker machines) store it as a WorkflowDefinition so that it can be executed directly from the Tailor Webapp. For this example we're just going to run the workflow directly: from pytailor import Project , FileSet , Workflow # open a project prj = Project . from_name ( \"Test\" ) # define inputs workflow_inputs = { \"pre_proc_data\" : [ { \"param1\" : 0 }, { \"param1\" : 1 }, { \"param1\" : 2 }, { \"param1\" : 3 }, { \"param1\" : 4 }, ] } # create a fileset and upload input files fileset = FileSet ( prj ) fileset . upload ( base_file = [ \"testfiles/testfile.inp\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"simulation workflow\" , inputs = workflow_inputs , fileset = fileset ) # run the workflow wf . run () Here we have introduced three new classes from the pytailor API: Project . A Tailor workflow has to be run in the context of a project. FileSet . Represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run. Workflow . Represents a single workflow run . In order to run a workflow we first instantiate the Workflow object, and then we call the Workflow.run method, which will start executing the workflow in the current python process. Note The direct mode of execution used here is handy when developing new workflows and for testing and debugging of new DAGs. For production workflow runs, distributed mode is suitable. See the worker tutorial for more information. Once the workflow has been started it can be monitored from the Tailor webapp. Below is shown how the workflow can be found in the list of workflows for the specific project by searching for the workflow name. When the workflow is selected the workflow files appear on the right side for inspection and direct download. By clicking on the Details link the workflow can be further inspected in the details view. Get started with pyTailor Head over to the Getting started section for instructions on how to setup pytailor. Once you are setup you can start working through the tutorials . You can also consult the API Reference for documentation of the pytailor API.","title":"Home"},{"location":"#welcome-to-the-pytailor-documentation","text":"","title":"Welcome to the pyTailor documentation"},{"location":"#what-is-pytailor","text":"Pytailor is a python client for the Tailor automation and collaboration platform. See tailor.wf for more information. With the pytailor API you can: Turn your existing python code into parameterized and reusable workflows. Run workflows locally in your computer or distributed to dedicated worker nodes. Store your workflows as workflow definitions in the Tailor backend and share it with others.","title":"What is pytailor?"},{"location":"#how-does-it-work","text":"Let's look at a motivational example. Say you have developed a set of python functions to solve some complex computing or engineering problem, and you have organized these functions into a python module called engineering_tasks.py . Say you have a three-step workflow like this: prepare input data for a set of simulations. This is handled by the function engineering_tasks.prepare_simulation_data . Let's say this function: takes a base input file as input. takes a list of parameter dicts for each simulation. produces one simulation input file for each parameter dict. run a set of simulations in parallel. A single simulation is handled by the function engineering_tasks.run_simulation . Let's say this function: takes a .inp file as input produces a .res file as output do post-processing of the simulation data. This is handled by the function engineering_tasks.post_process_simulation_data . Let's say this function: takes a set of .res files as input produces a file report.pdf returns a dict with essential post-processing results With pytailor you can wrap these functions into PythonTasks , and then use a DAG to define how these tasks relate to each other: from pytailor import PythonTask , BranchTask , DAG , Inputs , Outputs , Files import engineering_tasks inputs = Inputs () outputs = Outputs () files = Files () with DAG ( name = \"Advanced simulation dag\" ) as dag : t1 = PythonTask ( name = \"Pre-processing\" , function = engineering_tasks . prepare_simulation_data , kwargs = { \"parameters\" : inputs . pre_proc_data , \"base_file\" : files . base_file }, download = files . base_file , upload = { files . inp_file : \"sim_inp_file_*.inp\" } ) with BranchTask ( name = \"Parallel simulations\" , branch_files = files . inp_file , parents = t1 ) as branch : PythonTask ( name = \"Simulation\" , function = engineering_tasks . run_simulation , args = [ files . inp_file [ 0 ]], download = files . inp_file , upload = { files . res_file : \"*.res\" } ) PythonTask ( name = \"Post-processing\" , function = engineering_tasks . post_process_simulation_data , args = [ files . res_file ], download = files . res_file , upload = { files . report : \"report.pdf\" }, output_to = outputs . essential_results , parents = branch ) The DAG object represents the recipe for how the the computations shall be performed. By instantiating a DAG no computations are performed, note that we are just referencing the functions we want to use, we are not calling them. DAG is short for Directed Asyclic Graph , a term used to describe the logical flow of computations in a workflow. The DAG defined above is visualized below: A key feature in this DAG is the use of BranchTask to achieve parallelization or \"fan-out\" functionality. The term branching is used to describe this functionality, where one branch is created for each simulation. The Inputs , Outputs and Files objects are helper-objects for parameterization . When we e.g. say kwargs={\"parameters\": inputs.pre_proc_data} we are specifying that the value for the \"parameters\" keyword argument is parameterized and shall be looked up from the pre_proc_data name in the workflow's inputs when the task is executed. The concept of parameterization becomes clearer when we see how inputs , outputs and files are defined when we run a Workflow below. We now have a parameterized DAG describing the recipe of how we want to perform our computing workflow. Based on this definition we can run a Workflow , and we have several options: run it directly run it distributed (i.e in parallel, and optionally on several worker machines) store it as a WorkflowDefinition so that it can be executed directly from the Tailor Webapp. For this example we're just going to run the workflow directly: from pytailor import Project , FileSet , Workflow # open a project prj = Project . from_name ( \"Test\" ) # define inputs workflow_inputs = { \"pre_proc_data\" : [ { \"param1\" : 0 }, { \"param1\" : 1 }, { \"param1\" : 2 }, { \"param1\" : 3 }, { \"param1\" : 4 }, ] } # create a fileset and upload input files fileset = FileSet ( prj ) fileset . upload ( base_file = [ \"testfiles/testfile.inp\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"simulation workflow\" , inputs = workflow_inputs , fileset = fileset ) # run the workflow wf . run () Here we have introduced three new classes from the pytailor API: Project . A Tailor workflow has to be run in the context of a project. FileSet . Represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run. Workflow . Represents a single workflow run . In order to run a workflow we first instantiate the Workflow object, and then we call the Workflow.run method, which will start executing the workflow in the current python process. Note The direct mode of execution used here is handy when developing new workflows and for testing and debugging of new DAGs. For production workflow runs, distributed mode is suitable. See the worker tutorial for more information. Once the workflow has been started it can be monitored from the Tailor webapp. Below is shown how the workflow can be found in the list of workflows for the specific project by searching for the workflow name. When the workflow is selected the workflow files appear on the right side for inspection and direct download. By clicking on the Details link the workflow can be further inspected in the details view.","title":"How does it work?"},{"location":"#get-started-with-pytailor","text":"Head over to the Getting started section for instructions on how to setup pytailor. Once you are setup you can start working through the tutorials . You can also consult the API Reference for documentation of the pytailor API.","title":"Get started with pyTailor"},{"location":"api/parameterization/","text":"Parameterization Classes Inputs class pytailor. Inputs ( ) Helper object for inputs parameterization. Basic usage from pytailor import PythonTask , DAG , Inputs , Workflow , Project inputs = Inputs () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = inputs . hello , ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , inputs = { \"hello\" : [ \"Hello, world!\" ]} ) wf . run () Outputs class pytailor. Outputs ( ) Helper object for outputs parameterization. Basic usage from pytailor import PythonTask , DAG , Outputs , Workflow , Project outputs = Outputs () with DAG () as dag : t1 = PythonTask ( function = 'os.getcwd' , output_to = outputs . curdir ) PythonTask ( function = 'builtins.print' , args = [ outputs . curdir ], ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , ) wf . run () print ( wf . outputs ) Files class pytailor. Files ( ) Helper object for files parameterization. Basic usage from pytailor import PythonTask , DAG , Files , Workflow , Project , FileSet files = Files () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = [ \"This tasks download the file:\" , files . inpfile [ 0 ]], download = files . inpfile ) prj = Project . from_name ( \"Test\" ) fileset = FileSet ( project = prj ) fileset . upload ( inpfile = [ \"my_file.txt\" ]) wf = Workflow ( dag = dag , project = prj , fileset = fileset ) wf . run ()","title":"Parameterization classes"},{"location":"api/parameterization/#parameterization-classes","text":"","title":"Parameterization Classes"},{"location":"api/parameterization/#inputs","text":"class pytailor. Inputs ( ) Helper object for inputs parameterization. Basic usage from pytailor import PythonTask , DAG , Inputs , Workflow , Project inputs = Inputs () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = inputs . hello , ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , inputs = { \"hello\" : [ \"Hello, world!\" ]} ) wf . run ()","title":"Inputs"},{"location":"api/parameterization/#outputs","text":"class pytailor. Outputs ( ) Helper object for outputs parameterization. Basic usage from pytailor import PythonTask , DAG , Outputs , Workflow , Project outputs = Outputs () with DAG () as dag : t1 = PythonTask ( function = 'os.getcwd' , output_to = outputs . curdir ) PythonTask ( function = 'builtins.print' , args = [ outputs . curdir ], ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , ) wf . run () print ( wf . outputs )","title":"Outputs"},{"location":"api/parameterization/#files","text":"class pytailor. Files ( ) Helper object for files parameterization. Basic usage from pytailor import PythonTask , DAG , Files , Workflow , Project , FileSet files = Files () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = [ \"This tasks download the file:\" , files . inpfile [ 0 ]], download = files . inpfile ) prj = Project . from_name ( \"Test\" ) fileset = FileSet ( project = prj ) fileset . upload ( inpfile = [ \"my_file.txt\" ]) wf = Workflow ( dag = dag , project = prj , fileset = fileset ) wf . run ()","title":"Files"},{"location":"api/schema/","text":"Schema Definition Classes InputsSchema class pytailor. InputsSchema ( inputs ) Generator for inputsschema to define workflow definition Basic usage Following example define a jsonschema, that sets \"print\" as a required property for inputs and that the value must be a string example_inputs = { 'print' : 'Hello, world!' } inputsschema = InputsSchema ( inputs = example_inputs ) Add 'Hello, world' as a default for property 'print' example_inputs = { 'print' : 'Hello, world!' } inputsschema . add_defaults ( example_inputs ) # Set 'Hello, world' and 'Hello, tailor!' as only allowed alternatives for property 'print': enum_inputs = { 'print' : [ 'Hello, world!' , 'Hello, tailor!' ]} inputsschema . add_enums ( enum_inputs ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_defaults ( self , default_inputs ) Parameters default_inputs (dict) S Specify an example input schema that is valid for your workflow definition with defaults. add_enums ( self , enum_inputs ) Parameters enum_inputs (dict) S Specify altenatives for your inputsschema with a input schema where the property's values in a list represents the alternatives to_dict ( self ) Serialize input schema to_json ( self , filename , indent=4 ) a json file FilesSchema class pytailor. FilesSchema ( tags=None , exts=None , multiples=None , requireds=None , titles=None , descriptions=None ) Generator for filesschema to define workflow definition Basic usage files_ex1 = { \"tag\" : \"my_tag\" , \"ext\" : [ \"txt\" ], } filesschema = FilesSchema () filesschema . add_file ( tag = \"my_tag\" , ext = [ \"txt\" ]) files_ex2 = { \"tag\" : \"my__new_tag\" , \"ext\" : [ \"txt\" ], \"title\" : \"Coordinates\" , \"multiple\" : True , \"description\" : \"A file with coordinate values of nodes\" , \"required\" : True , } filesschema = FilesSchema () filesschema . add_file ( ** files_ex2 ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_file ( self , tag , ext=None , multiple=False , required=True , title=None , description='' ) Parameters tag (str) Specify tag for file. tag (str) Specify allowed extension for file mutiple (bool) Specify whether multiples files are allowed required (bool) Specify whether file(s) are required title (str) Specify the title to be shown in GUI description (str) Specify the description to be shown in GUI to_dict ( self ) Serialize files schema to_json ( self , filename , indent=4 ) Description class pytailor. Description ( name=None , description_string=None ) Workflow definition description generator to define workflow definition Basic usage wf_def_description = \"This example explains branchtasks\" wf_def_name = \"branch task example\" from pytailor import PythonTask , BranchTask , DAG with DAG ( name = \"duplicate dag example\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"* / .txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) description = Description . from_dag ( dag , wf_def_name = wf_def_name , wf_def_description = wf_def_description ) Parameters name (dict) Specify a name for the workflow definition description_string (str) Specify a description string in markdown format from_dag ( dag , wf_def_name='' , wf_def_description='' ) Generates a workflow definition description from a pytailor DAG class Parameters dag (DAG) Specify your DAG for workflow definition wf_def_name (str) Specify a name for workflow definition wf_def_description (str) Specify an overall description, what are the main objectives of your workflow definition? What are the main steps in the DAG? to_string ( self ) to_markdown ( self , filename='Readme.MD' )","title":"Schema definition classes"},{"location":"api/schema/#schema-definition-classes","text":"","title":"Schema Definition Classes"},{"location":"api/schema/#inputsschema","text":"class pytailor. InputsSchema ( inputs ) Generator for inputsschema to define workflow definition Basic usage Following example define a jsonschema, that sets \"print\" as a required property for inputs and that the value must be a string example_inputs = { 'print' : 'Hello, world!' } inputsschema = InputsSchema ( inputs = example_inputs ) Add 'Hello, world' as a default for property 'print' example_inputs = { 'print' : 'Hello, world!' } inputsschema . add_defaults ( example_inputs ) # Set 'Hello, world' and 'Hello, tailor!' as only allowed alternatives for property 'print': enum_inputs = { 'print' : [ 'Hello, world!' , 'Hello, tailor!' ]} inputsschema . add_enums ( enum_inputs ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_defaults ( self , default_inputs ) Parameters default_inputs (dict) S Specify an example input schema that is valid for your workflow definition with defaults. add_enums ( self , enum_inputs ) Parameters enum_inputs (dict) S Specify altenatives for your inputsschema with a input schema where the property's values in a list represents the alternatives to_dict ( self ) Serialize input schema to_json ( self , filename , indent=4 ) a json file","title":"InputsSchema"},{"location":"api/schema/#filesschema","text":"class pytailor. FilesSchema ( tags=None , exts=None , multiples=None , requireds=None , titles=None , descriptions=None ) Generator for filesschema to define workflow definition Basic usage files_ex1 = { \"tag\" : \"my_tag\" , \"ext\" : [ \"txt\" ], } filesschema = FilesSchema () filesschema . add_file ( tag = \"my_tag\" , ext = [ \"txt\" ]) files_ex2 = { \"tag\" : \"my__new_tag\" , \"ext\" : [ \"txt\" ], \"title\" : \"Coordinates\" , \"multiple\" : True , \"description\" : \"A file with coordinate values of nodes\" , \"required\" : True , } filesschema = FilesSchema () filesschema . add_file ( ** files_ex2 ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_file ( self , tag , ext=None , multiple=False , required=True , title=None , description='' ) Parameters tag (str) Specify tag for file. tag (str) Specify allowed extension for file mutiple (bool) Specify whether multiples files are allowed required (bool) Specify whether file(s) are required title (str) Specify the title to be shown in GUI description (str) Specify the description to be shown in GUI to_dict ( self ) Serialize files schema to_json ( self , filename , indent=4 )","title":"FilesSchema"},{"location":"api/schema/#description","text":"class pytailor. Description ( name=None , description_string=None ) Workflow definition description generator to define workflow definition Basic usage wf_def_description = \"This example explains branchtasks\" wf_def_name = \"branch task example\" from pytailor import PythonTask , BranchTask , DAG with DAG ( name = \"duplicate dag example\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"* / .txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) description = Description . from_dag ( dag , wf_def_name = wf_def_name , wf_def_description = wf_def_description ) Parameters name (dict) Specify a name for the workflow definition description_string (str) Specify a description string in markdown format from_dag ( dag , wf_def_name='' , wf_def_description='' ) Generates a workflow definition description from a pytailor DAG class Parameters dag (DAG) Specify your DAG for workflow definition wf_def_name (str) Specify a name for workflow definition wf_def_description (str) Specify an overall description, what are the main objectives of your workflow definition? What are the main steps in the DAG? to_string ( self ) to_markdown ( self , filename='Readme.MD' )","title":"Description"},{"location":"api/taskdefs/","text":"Task definition classes PythonTask class pytailor. PythonTask ( function , name=None , parents=None , owner=None , download=None , upload=None , args=None , kwargs=None , output_to=None , output_extraction=None , use_storage_dirs=True , requirements=None ) Task for running python code. Basic usage pytask = PythonTask ( function = 'builtins.print' , args = 'Hello, world!' , name = 'My first task' ) Parameters function (str or Callable) Python callable. Must be importable in the executing python env. name (str, optional) A default name is used if not provided. parents (BaseTask or List[BaseTask], optional) Specify one or more upstream tasks that this task depends on. download (str, list or Parameterization, optional) Provide one or more file tags. These file tags refer to files in the storage object associated with the workflow run. upload (dict, optional) Specify files to send back to the storage object after a task has been run. Dict format is {tag1: val1, tag2: val2, ...} where val can be: one or more query expressions(str og list) which is applied to the return value from callable . File names resulting from the query are then uploaded to storage under the given tag. one or more glob-style strings (str og list) which is applied in the task working dir. matching files are uploaded under the given tag. args (list, str or Parameterization, optional) Arguments to be passed as positional arguments to function . Accepts a list of ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a list. See the examples for how parameterization objects and query expressions are used. kwargs (dict, str or Parameterization, optional) Arguments to be passed as keyword arguments to function . accepts a kwargs dict where values can be ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a dict. See the examples for how parameterization objects and query expressions are used. output_to (str or Parameterization, optional) The return value of the callable is stored under the provided name in the workflow outputs . This value is then available for downstream task. output_extraction (dict, optional) Provide a dict of name: expr where expr are query-expressions to extract parts of the return value of the callable. The keys of the dict are used as names for storing in the workflow outputs which becomes available for downstream tasks. use_storage_dirs: (boolean, Optional) If True (default) files downloaded from storage are stored in a folder structure mirroring the storage If False files downloaded from storage are stored \"flat\" in the current directory requirements: (list, optional) A list of requirements this task places on the worker environment to_dict ( self ) Serialize task definition. from_dict ( d ) Create from serialized task definition. DAG class pytailor. DAG ( tasks=None , name=None , parents=None , owner=None , links=None , requirements=None ) Represents a Directed Acyclic Graph, i.e. a DAG. Parameters tasks : BaseTask or List[BaseTask] PythonTask, BranchTask or DAG objects. name : str, optional A default name is used if not provided. parents : BaseTask or List[BaseTask], optional Specify one or more upstream tasks that this task depends on. links : dict, optional Parent/children relationships can be specified with the dict on the form {parent_def: [child_def1, child_def2], ...}. Definition references may either be indices (ints) into tasks or BaseTask instances. Note that links may also be defined on task objects with the parents argument instead of using links: (parents=[parent_def1, parent_def2]) requirements: (list, optional) A list of requirements this task places on the worker environment to_dict ( self ) from_dict ( d ) BranchTask class pytailor. BranchTask ( task=None , name=None , parents=None , owner=None , branch_data=None , branch_files=None , requirements=None ) Dynamically branch a task or DAG during workflow execution. BranchTask Provides parallelization or \"fan-out\" functionality. The task is duplicated based on branch_data or branch_files . At least one of these must be specified. Parameters task : BaseTask Task to be duplicated (PythonTask, BranchTask or DAG). name : str, optional A default name is used if not provided. parents : BaseTask or List[BaseTask], optional Specify one or more upstream tasks that this task depends on. branch_data : list or str, optional Data to be used as basis for branching. Accepts a query-expression or a list of query-expressions. The queries must evaluate to a list or a dict. If the query evaluates to a dict, that dict must have integer keys to represent the index of each branch. branch_files : list or str, optional Files to be used as basis for branching. Accepts a file tag or a list of file tags. requirements: (list, optional) A list of requirements this task places on the worker environment to_dict ( self ) from_dict ( d )","title":"Task definition classes"},{"location":"api/taskdefs/#task-definition-classes","text":"","title":"Task definition classes"},{"location":"api/taskdefs/#pythontask","text":"class pytailor. PythonTask ( function , name=None , parents=None , owner=None , download=None , upload=None , args=None , kwargs=None , output_to=None , output_extraction=None , use_storage_dirs=True , requirements=None ) Task for running python code. Basic usage pytask = PythonTask ( function = 'builtins.print' , args = 'Hello, world!' , name = 'My first task' ) Parameters function (str or Callable) Python callable. Must be importable in the executing python env. name (str, optional) A default name is used if not provided. parents (BaseTask or List[BaseTask], optional) Specify one or more upstream tasks that this task depends on. download (str, list or Parameterization, optional) Provide one or more file tags. These file tags refer to files in the storage object associated with the workflow run. upload (dict, optional) Specify files to send back to the storage object after a task has been run. Dict format is {tag1: val1, tag2: val2, ...} where val can be: one or more query expressions(str og list) which is applied to the return value from callable . File names resulting from the query are then uploaded to storage under the given tag. one or more glob-style strings (str og list) which is applied in the task working dir. matching files are uploaded under the given tag. args (list, str or Parameterization, optional) Arguments to be passed as positional arguments to function . Accepts a list of ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a list. See the examples for how parameterization objects and query expressions are used. kwargs (dict, str or Parameterization, optional) Arguments to be passed as keyword arguments to function . accepts a kwargs dict where values can be ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a dict. See the examples for how parameterization objects and query expressions are used. output_to (str or Parameterization, optional) The return value of the callable is stored under the provided name in the workflow outputs . This value is then available for downstream task. output_extraction (dict, optional) Provide a dict of name: expr where expr are query-expressions to extract parts of the return value of the callable. The keys of the dict are used as names for storing in the workflow outputs which becomes available for downstream tasks. use_storage_dirs: (boolean, Optional) If True (default) files downloaded from storage are stored in a folder structure mirroring the storage If False files downloaded from storage are stored \"flat\" in the current directory requirements: (list, optional) A list of requirements this task places on the worker environment to_dict ( self ) Serialize task definition. from_dict ( d ) Create from serialized task definition.","title":"PythonTask"},{"location":"api/taskdefs/#dag","text":"class pytailor. DAG ( tasks=None , name=None , parents=None , owner=None , links=None , requirements=None ) Represents a Directed Acyclic Graph, i.e. a DAG.","title":"DAG"},{"location":"api/taskdefs/#branchtask","text":"class pytailor. BranchTask ( task=None , name=None , parents=None , owner=None , branch_data=None , branch_files=None , requirements=None ) Dynamically branch a task or DAG during workflow execution. BranchTask Provides parallelization or \"fan-out\" functionality. The task is duplicated based on branch_data or branch_files . At least one of these must be specified.","title":"BranchTask"},{"location":"api/workflow/","text":"Workflow class Workflow class pytailor. Workflow ( project , dag , name=None , inputs=None , fileset=None ) The Workflow class is used to create new workflows or operate on existing workflows. Instantiation patterns: - To create a new workflow from a DAG use the default constructor - To create a new workflow from a workflow definition use Workflow.from_definition_id() - To retrieve a workflow from the backend use Workflow.from_project_and_id() run ( self , distributed=False , worker_name=None ) Start the workflow. Parameters distributed (bool, Optional) If False (default) the workflow is executed immediately in the current python process one task at a time. Useful for development and debugging. If True the workflow will be launched to the database, and tasks will be executed in parallel on one or more workers. worker_name (str, Optional) A worker name can be provided to control which worker(s) will execute the workflow's tasks. This parameter is ignored for distributed=False","title":"Workflow class"},{"location":"api/workflow/#workflow-class","text":"","title":"Workflow class"},{"location":"api/workflow/#workflow","text":"class pytailor. Workflow ( project , dag , name=None , inputs=None , fileset=None ) The Workflow class is used to create new workflows or operate on existing workflows. Instantiation patterns: - To create a new workflow from a DAG use the default constructor - To create a new workflow from a workflow definition use Workflow.from_definition_id() - To retrieve a workflow from the backend use Workflow.from_project_and_id() run ( self , distributed=False , worker_name=None ) Start the workflow. Parameters distributed (bool, Optional) If False (default) the workflow is executed immediately in the current python process one task at a time. Useful for development and debugging. If True the workflow will be launched to the database, and tasks will be executed in parallel on one or more workers. worker_name (str, Optional) A worker name can be provided to control which worker(s) will execute the workflow's tasks. This parameter is ignored for distributed=False","title":"Workflow"},{"location":"api/workflow_definition/","text":"Workflow definition class Workflow definition class pytailor. WorkflowDefinition ( name , description , dag , inputs_schema=None , outputs_schema=None , files_schema=None ) Create a new workflow definition. Workflow Definitions are parameterized and reusable blueprints for computing workflows. Parameters name str, optional Provide a name for this workflow definition. dag : DAG Provide a dag object for this workflow definition. inputs_schema dict, optional JSON-schema for rendering of inputs in the react web app and validating the structure and data typing of the input parameters (dict) outputs_schema dict, optional JSON-schema for validating the structure and data typing of the output parameters (dict) files_schema dict, optional JSON-file for rendering of file upload tab in the react web app and validating file extensions etc. add_to_account ( self , account )","title":"Workflow definition class"},{"location":"api/workflow_definition/#workflow-definition-class","text":"","title":"Workflow definition class"},{"location":"api/workflow_definition/#workflow-definition","text":"class pytailor. WorkflowDefinition ( name , description , dag , inputs_schema=None , outputs_schema=None , files_schema=None ) Create a new workflow definition. Workflow Definitions are parameterized and reusable blueprints for computing workflows. Parameters name str, optional Provide a name for this workflow definition. dag : DAG Provide a dag object for this workflow definition. inputs_schema dict, optional JSON-schema for rendering of inputs in the react web app and validating the structure and data typing of the input parameters (dict) outputs_schema dict, optional JSON-schema for validating the structure and data typing of the output parameters (dict) files_schema dict, optional JSON-file for rendering of file upload tab in the react web app and validating file extensions etc. add_to_account ( self , account )","title":"Workflow definition"},{"location":"documentation/account_management/","text":"Tailor Backend The data, files, tasks, workflows and workers are orchestrated by the tailor backend. A config file with a valid user API key must exist, to let the pytailor client communicate with the backend. User accounts You can request a free user account at tailor.wf . The user account lets you access the web app, with an overview of your projects, workflows and workflow definitions. Projects A workflow must belong to a Project, where the workflow, including tasks, files and data is stored. For the basic user registration two default projects are created for you: Project Test is your private project, set up to let you experiment on your own. Project Prod is set up to allow users on the same account to collaborate. Workers may also be set up to only run tasks that belong to a specific project. Accounts To share your defined workflow ( workflow definition ) with other users in your team, a workflow definition must be added to your account. The account is shared between the different users in your organization or workgroup. Your colleagues in your account may also start a new workflow from your workflow definitions. Workflow definition subscriptions You may run workflows based on definitions from other accounts. This is done by adding the id of the other account's workflow definition to your account's workflow definition subscription list . Vise versa you may allow others to subscribe to your workflow definitions. In both cases the workflows are run on the workers belonging to the creator, such that the code is not exposed to the subscriber.","title":"Tailor backend"},{"location":"documentation/account_management/#tailor-backend","text":"The data, files, tasks, workflows and workers are orchestrated by the tailor backend. A config file with a valid user API key must exist, to let the pytailor client communicate with the backend.","title":"Tailor Backend"},{"location":"documentation/account_management/#user-accounts","text":"You can request a free user account at tailor.wf . The user account lets you access the web app, with an overview of your projects, workflows and workflow definitions.","title":"User accounts"},{"location":"documentation/account_management/#projects","text":"A workflow must belong to a Project, where the workflow, including tasks, files and data is stored. For the basic user registration two default projects are created for you: Project Test is your private project, set up to let you experiment on your own. Project Prod is set up to allow users on the same account to collaborate. Workers may also be set up to only run tasks that belong to a specific project.","title":"Projects"},{"location":"documentation/account_management/#accounts","text":"To share your defined workflow ( workflow definition ) with other users in your team, a workflow definition must be added to your account. The account is shared between the different users in your organization or workgroup. Your colleagues in your account may also start a new workflow from your workflow definitions.","title":"Accounts"},{"location":"documentation/account_management/#workflow-definition-subscriptions","text":"You may run workflows based on definitions from other accounts. This is done by adding the id of the other account's workflow definition to your account's workflow definition subscription list . Vise versa you may allow others to subscribe to your workflow definitions. In both cases the workflows are run on the workers belonging to the creator, such that the code is not exposed to the subscriber.","title":"Workflow definition subscriptions"},{"location":"documentation/concepts/","text":"Programming workflows DAGs DAGs are used to define relationships between tasks in a Workflow in the form of a Directed Acyclic Graph. That the DAG is \"Directed\" means that the parent task is computed before the child task. That the DAG is \"Acyclic\" means that there is no way to start at any task t and follow a consistently-directed sequence of tasks that eventually loops back to t again. In other words there are no loops in the workflow. This means that strictly speaking an iteration loop (while loop) is not allowed in a Tailor workflow. Since iteration loops are a necessary part of many engineering workflows, we are currently developing a WhileTask. A WhileTask has an internal loop that runs until a condition is met. The child of the WhileTask only \"sees\" the resulting output and thus the WhileTask behaves just as any other task in the workflow. Task definitions Task definitions are parameterized and reusable blueprints for computing tasks. In pytailor, tasks definitions are created using the task definition classes The available task definition classes are: PythonTask This is the basic building block, used to define the execution of a single Python function (callable). BranchTask This is the basic branching building block. It creates branches based on files or data. WhileTask (in development) This is the basic iteration building block. It runs the same task over and over with updated input data until a condition is met. By combining the different task types, arbitrary complex tasks can be defined. A non-trivial task definition will typically consist of a DAG at the top-level, which in turn consist of other task definitions as illustrated in tutorials 6 , 7 and 9 . Note Tailor is still in development and more task definition classes are likely to be introduced in the future to extend functionality. Workflows Workflows are instantiated DAGs with a given set of inputs and files. Workflows are stored on the Tailor backend under a given Project . dag project inputs (parameters, JSON) files (inputs files) worker requirements In pytailor, workflows are represented by the Workflow class Tasks Tasks are instantiated task definitions . Belongs to a Workflow . Workflow definitions Workflow definitions are used to store a DAG along with requirements (schema) for inputs and files. Workflow definitions are stored in the backend and can be made available for other users.","title":"Programming workflows"},{"location":"documentation/concepts/#programming-workflows","text":"","title":"Programming workflows"},{"location":"documentation/concepts/#dags","text":"DAGs are used to define relationships between tasks in a Workflow in the form of a Directed Acyclic Graph. That the DAG is \"Directed\" means that the parent task is computed before the child task. That the DAG is \"Acyclic\" means that there is no way to start at any task t and follow a consistently-directed sequence of tasks that eventually loops back to t again. In other words there are no loops in the workflow. This means that strictly speaking an iteration loop (while loop) is not allowed in a Tailor workflow. Since iteration loops are a necessary part of many engineering workflows, we are currently developing a WhileTask. A WhileTask has an internal loop that runs until a condition is met. The child of the WhileTask only \"sees\" the resulting output and thus the WhileTask behaves just as any other task in the workflow.","title":"DAGs"},{"location":"documentation/concepts/#task-definitions","text":"Task definitions are parameterized and reusable blueprints for computing tasks. In pytailor, tasks definitions are created using the task definition classes The available task definition classes are: PythonTask This is the basic building block, used to define the execution of a single Python function (callable). BranchTask This is the basic branching building block. It creates branches based on files or data. WhileTask (in development) This is the basic iteration building block. It runs the same task over and over with updated input data until a condition is met. By combining the different task types, arbitrary complex tasks can be defined. A non-trivial task definition will typically consist of a DAG at the top-level, which in turn consist of other task definitions as illustrated in tutorials 6 , 7 and 9 . Note Tailor is still in development and more task definition classes are likely to be introduced in the future to extend functionality.","title":"Task definitions"},{"location":"documentation/concepts/#workflows","text":"Workflows are instantiated DAGs with a given set of inputs and files. Workflows are stored on the Tailor backend under a given Project . dag project inputs (parameters, JSON) files (inputs files) worker requirements In pytailor, workflows are represented by the Workflow class","title":"Workflows"},{"location":"documentation/concepts/#tasks","text":"Tasks are instantiated task definitions . Belongs to a Workflow .","title":"Tasks"},{"location":"documentation/concepts/#workflow-definitions","text":"Workflow definitions are used to store a DAG along with requirements (schema) for inputs and files. Workflow definitions are stored in the backend and can be made available for other users.","title":"Workflow definitions"},{"location":"documentation/contexts/","text":"Contexts Contexts represents the data and files associated with a workflow. The context consist of three data structures: inputs outputs files inputs are provided by the user during workflow creation, e.g. from pytailor import PythonTask , DAG , Workflow , Project , Inputs import time ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = time . sleep , name = \"task 1\" , args = [ inputs . data . sleep_time [ 0 ]] ) t2 = PythonTask ( function = print , name = \"task 2\" , args = [ \" \\n Slept for\" , inputs . data , \"second\" ], kwargs = { \"sep\" : inputs . sep , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs wf_inputs = { \"data\" : { \"sleep_time\" : [ 1.5 ]}, \"sep\" : \" \" } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = wf_inputs ) Note The inputs and outputs data structures must be JSON-serializable, which limits the data types which can be used. In the future more sophisticated serialization may be applied to allow other object types, e.g. numpy arrays. For data that is not JSON-compatible you can serialize the data to file and use the file-piping mechanisms to send the data to your tasks. Fileset A fileset represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run.","title":"Contexts"},{"location":"documentation/contexts/#contexts","text":"Contexts represents the data and files associated with a workflow. The context consist of three data structures: inputs outputs files inputs are provided by the user during workflow creation, e.g. from pytailor import PythonTask , DAG , Workflow , Project , Inputs import time ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = time . sleep , name = \"task 1\" , args = [ inputs . data . sleep_time [ 0 ]] ) t2 = PythonTask ( function = print , name = \"task 2\" , args = [ \" \\n Slept for\" , inputs . data , \"second\" ], kwargs = { \"sep\" : inputs . sep , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs wf_inputs = { \"data\" : { \"sleep_time\" : [ 1.5 ]}, \"sep\" : \" \" } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = wf_inputs ) Note The inputs and outputs data structures must be JSON-serializable, which limits the data types which can be used. In the future more sophisticated serialization may be applied to allow other object types, e.g. numpy arrays. For data that is not JSON-compatible you can serialize the data to file and use the file-piping mechanisms to send the data to your tasks.","title":"Contexts"},{"location":"documentation/contexts/#fileset","text":"A fileset represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run.","title":"Fileset"},{"location":"documentation/getting_started/","text":"Getting started To use pytailor you need to connect to a Tailor backend. The easiest way to get started with Tailor is to sign up for a free account at tailor.wf . For other options please contact us . In the following it us assumed that you are using tailor.wf as your backend service. Installation First time setup 1. Install pytailor Official releases of pytailor are available on PyPI and can easily be installed with pip: pip install pytailor Note You can also clone the repository on github and install pytailor with poetry : git clone https://github.com/entailor/pytailor.git Then, from the project root run: poetry install To install without development dependencies: poetry install --no-dev 2. Configure backend When pytailor is installed you can use the CLI to set up a barebone config file: tailor init This command generates a config file .tailor/config.toml under your home directory with the following content: [pytailor] API_BASE_URL = \"<API BASE URL HERE>\" API_CLIENT_ID = \"<API CLIENT ID HERE>\" API_WORKER_ID = \"<API WORKER ID HERE>\" API_SECRET_KEY = \"<API KEY HERE>\" API_IDP_URL = \"<API IDP URL HERE>\" [worker.my_config] sleep = 3 ncores = 7 workername = \"my_worker\" project_ids = [] capabilities = [] Note It is also possible to configure pytailor with environmental variables by prefixing the environmental variables with PYTAILOR_ . E.g. to set the API key, put it in an environmental variable called PYTAILOR_API_SECRET_KEY . 3. Testing Once you are setup you can start working through the tutorials .","title":"Getting started"},{"location":"documentation/getting_started/#getting-started","text":"To use pytailor you need to connect to a Tailor backend. The easiest way to get started with Tailor is to sign up for a free account at tailor.wf . For other options please contact us . In the following it us assumed that you are using tailor.wf as your backend service.","title":"Getting started"},{"location":"documentation/getting_started/#installation","text":"","title":"Installation"},{"location":"documentation/getting_started/#first-time-setup","text":"","title":"First time setup"},{"location":"documentation/getting_started/#1-install-pytailor","text":"Official releases of pytailor are available on PyPI and can easily be installed with pip: pip install pytailor Note You can also clone the repository on github and install pytailor with poetry : git clone https://github.com/entailor/pytailor.git Then, from the project root run: poetry install To install without development dependencies: poetry install --no-dev","title":"1. Install pytailor"},{"location":"documentation/getting_started/#2-configure-backend","text":"When pytailor is installed you can use the CLI to set up a barebone config file: tailor init This command generates a config file .tailor/config.toml under your home directory with the following content: [pytailor] API_BASE_URL = \"<API BASE URL HERE>\" API_CLIENT_ID = \"<API CLIENT ID HERE>\" API_WORKER_ID = \"<API WORKER ID HERE>\" API_SECRET_KEY = \"<API KEY HERE>\" API_IDP_URL = \"<API IDP URL HERE>\" [worker.my_config] sleep = 3 ncores = 7 workername = \"my_worker\" project_ids = [] capabilities = [] Note It is also possible to configure pytailor with environmental variables by prefixing the environmental variables with PYTAILOR_ . E.g. to set the API key, put it in an environmental variable called PYTAILOR_API_SECRET_KEY .","title":"2. Configure backend"},{"location":"documentation/getting_started/#3-testing","text":"Once you are setup you can start working through the tutorials .","title":"3. Testing"},{"location":"documentation/workers/","text":"Worker A worker is a process that asks the Tailor backend for tasks that are ready. During runtime the worker process reports back the status of the task execution. Once the task is completed, the worker process is ready to execute a new tasks that is ready in the database. To start a worker, open a terminal in an environment where you have pytailor installed and write: tailor worker A worker with the default configuration as defined in the /.tailor/config.toml file will now start asking for tasks to execute. If you want to configure the worker to sleep for 5 seconds between each time it asks for work, using 10 cores on the computer. tailor worker --sleep 5 --ncores 10 For a complete list of possible worker configurations write tailor worker --help If you want to run several different workers with different configurations, simply open several terminals and repeat. Worker management Filter on project Workers may be configured to only run tasks in a workflow that belongs to a specific project . This is done by assigning a project at initiation: tailor worker --project-id-filter <your_project_id_filter ( project.id ) > Multiple projects may be assigned by repeating the configuration key: tailor worker --project-id-filter <id_1> --project-id-filter <id_2> Filter on worker name A workflow may be distributed to run only on workers with a given worker name. To run these workflows, the same worker name must be specified. tailor worker --workername <my_worker_name (string)> Example Workflow execution from pytailor import PythonTask , DAG , Project , Workflow with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"builtins.print\" , name = \"job 1\" , args = [ \"Hello, world!\" ], ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from pytailor\" , ) wf . run ( distributed = True , workername = 'my_worker' ) Configuring worker tailor worker --workername my_worker Filter on task requirement Tasks may require a certain capability for the worker environment, such as installed software. To tag a worker with the requirement, capability is applied at worker configuration. tailor worker --capability <specified_capability_requirement_in_task (string)> Example Task definition from pytailor import PythonTask t1 = PythonTask ( function = 'builtins.print' , name = 'capability demo' , args = [ 'hello capability!' ], requirements = [ 'key1' ] ) Configuring worker tailor worker --capability key1 Multiple capabilities may be assigned by repeating the configuration key: tailor worker --capability key1 --capability key2","title":"Workers"},{"location":"documentation/workers/#worker","text":"A worker is a process that asks the Tailor backend for tasks that are ready. During runtime the worker process reports back the status of the task execution. Once the task is completed, the worker process is ready to execute a new tasks that is ready in the database. To start a worker, open a terminal in an environment where you have pytailor installed and write: tailor worker A worker with the default configuration as defined in the /.tailor/config.toml file will now start asking for tasks to execute. If you want to configure the worker to sleep for 5 seconds between each time it asks for work, using 10 cores on the computer. tailor worker --sleep 5 --ncores 10 For a complete list of possible worker configurations write tailor worker --help If you want to run several different workers with different configurations, simply open several terminals and repeat.","title":"Worker"},{"location":"documentation/workers/#worker-management","text":"","title":"Worker management"},{"location":"documentation/workers/#filter-on-project","text":"Workers may be configured to only run tasks in a workflow that belongs to a specific project . This is done by assigning a project at initiation: tailor worker --project-id-filter <your_project_id_filter ( project.id ) > Multiple projects may be assigned by repeating the configuration key: tailor worker --project-id-filter <id_1> --project-id-filter <id_2>","title":"Filter on project"},{"location":"documentation/workers/#filter-on-worker-name","text":"A workflow may be distributed to run only on workers with a given worker name. To run these workflows, the same worker name must be specified. tailor worker --workername <my_worker_name (string)>","title":"Filter on worker name"},{"location":"documentation/workers/#example","text":"Workflow execution from pytailor import PythonTask , DAG , Project , Workflow with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"builtins.print\" , name = \"job 1\" , args = [ \"Hello, world!\" ], ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from pytailor\" , ) wf . run ( distributed = True , workername = 'my_worker' ) Configuring worker tailor worker --workername my_worker","title":"Example"},{"location":"documentation/workers/#filter-on-task-requirement","text":"Tasks may require a certain capability for the worker environment, such as installed software. To tag a worker with the requirement, capability is applied at worker configuration. tailor worker --capability <specified_capability_requirement_in_task (string)>","title":"Filter on task requirement"},{"location":"documentation/workers/#example_1","text":"Task definition from pytailor import PythonTask t1 = PythonTask ( function = 'builtins.print' , name = 'capability demo' , args = [ 'hello capability!' ], requirements = [ 'key1' ] ) Configuring worker tailor worker --capability key1 Multiple capabilities may be assigned by repeating the configuration key: tailor worker --capability key1 --capability key2","title":"Example"},{"location":"tutorials/example01_hello_world/","text":"This is the Hello world example for pytailor. This example introduces the following NEW concepts: Create PythonTasks and DAGs For PythonTasks: Specifying the function to run (must be an importable python function) Specifying a name for the task Specifying positional arguments (*args) to the function Specifying relationships between tasks For DAGs: Specifying which tasks are part of the DAG Specifying a name for the DAG Create a Workflow and run it with default settings ( distributed=False ) Check status of the resulting Workflow after execution Retrieve a workflow from the backend into a new Workflow object from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) dag = DAG ( tasks = [ t1 , t2 ], name = \"dag\" ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"Tutorial 1"},{"location":"tutorials/example01_hello_world_alt_syntax1/","text":"pyTailor Tutorial 1, alternative syntax 1 Here is the Hello world example with alternative syntax allowing for definition of the DAG in \"reversed\" order by using the owner parameter. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### dag = DAG ( name = \"dag\" ) t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], owner = dag , ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , owner = dag , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"Example01 hello world alt syntax1"},{"location":"tutorials/example01_hello_world_alt_syntax1/#pytailor-tutorial-1-alternative-syntax-1","text":"Here is the Hello world example with alternative syntax allowing for definition of the DAG in \"reversed\" order by using the owner parameter. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### dag = DAG ( name = \"dag\" ) t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], owner = dag , ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , owner = dag , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"pyTailor Tutorial 1, alternative syntax 1"},{"location":"tutorials/example01_hello_world_alt_syntax2/","text":"pyTailor Tutorial 1, alternative syntax 2 Here is the Hello world example with alternative syntax using context managers. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"Example01 hello world alt syntax2"},{"location":"tutorials/example01_hello_world_alt_syntax2/#pytailor-tutorial-1-alternative-syntax-2","text":"Here is the Hello world example with alternative syntax using context managers. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"pyTailor Tutorial 1, alternative syntax 2"},{"location":"tutorials/example02_kwargs/","text":"This example introduces the following NEW concepts: For PythonTasks: Specifying keyword arguments (**kwargs) to the function from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"time.sleep\" , name = \"task 1\" , args = [ 1 ]) t2 = PythonTask ( function = \"builtins.print\" , name = \"task 2\" , args = [ \" \\n Slept for\" , \"1\" , \"second\" ], kwargs = { \"sep\" : \" \" , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"kwarg workflow\" ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Tutorial 2"},{"location":"tutorials/example03_inputs/","text":"This example introduces the following NEW concepts: For task definitions: Using query expressions. Specifying inputs when creating a workflow Query expressions is a means to parameterize the inputs that are specified in a task definition. The query will be applied later when the workflow is executed and the parameters to use will be extracted from the input provided to that specific workflow. To use a query, the query string must be on the format \"<% query %>\". The yaql python package is used for handling queries, see https://yaql.readthedocs.io/en/latest/index.html. Inputs are immutable, in the sense that they cannot be changed during the execution of the workflow. Note Currently this mechanism only works with data that is directly serializable. In the future non-JSON compatible objects may be handled as well (by use of pickling). from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"time.sleep\" , name = \"task 1\" , args = [ \"<% $.inputs.sleep_time %>\" ] ) t2 = PythonTask ( function = \"builtins.print\" , name = \"task 2\" , args = [ \" \\n Slept for\" , \"<% $.inputs.sleep_time %>\" , \"second\" ], kwargs = { \"sep\" : \" \" , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs inputs = { \"sleep_time\" : 1.5 } # try to change this and rerun the workflow # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"Tutorial 3"},{"location":"tutorials/example03_inputs_parameterization/","text":"pyTailor Tutorial 3 from pytailor import PythonTask , DAG , Workflow , Project , Inputs import time ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = time . sleep , name = \"task 1\" , args = [ inputs . data . sleep_time [ 0 ]] ) t2 = PythonTask ( function = print , name = \"task 2\" , args = [ \" \\n Slept for\" , inputs . data , \"second\" ], kwargs = { \"sep\" : inputs . sep , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs wf_inputs = { \"data\" : { \"sleep_time\" : [ 1.5 ]}, \"sep\" : \" \" } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"Example03 inputs parameterization"},{"location":"tutorials/example03_inputs_parameterization/#pytailor-tutorial-3","text":"from pytailor import PythonTask , DAG , Workflow , Project , Inputs import time ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = time . sleep , name = \"task 1\" , args = [ inputs . data . sleep_time [ 0 ]] ) t2 = PythonTask ( function = print , name = \"task 2\" , args = [ \" \\n Slept for\" , inputs . data , \"second\" ], kwargs = { \"sep\" : inputs . sep , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs wf_inputs = { \"data\" : { \"sleep_time\" : [ 1.5 ]}, \"sep\" : \" \" } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"pyTailor Tutorial 3"},{"location":"tutorials/example04_outputs/","text":"This example introduces the following NEW concepts: For PythonTask definitions: Specifying what to do with function output Accessing function output in downstream tasks Specifying multiple parents The output argument to PythonTask can be specified in two forms: A single string. Then the entire return value of the function is put on $.outputs. A dictionary. Then for each (tag: query) in the dict the query is applied to the return value and the result is put on $.outputs. The output can be accessed in downstream tasks using query expressions like \"<% $.outputs.<tag> %>\" . The output is also available as an attribute (dict) on the workflow run objects retrieved from the database (WorkflowRun.outputs). Note Currently this mechanism only work with data that is directly serializable. In the future non-JSON compatible objects will be handled as well (by use of pickling). from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 1\" , args = [ \"../*.py\" ], output_to = \"parentdir_content\" , # form 1: single string ) t2 = PythonTask ( function = \"os.getcwd\" , name = \"task 2\" , output_extraction = { \"curdir\" : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , \"<% $.outputs.parentdir_content %>\" , \"Current working dir:\" , \"<% $.outputs.curdir %>\" , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"Tutorial 4"},{"location":"tutorials/example04_outputs_parameterization/","text":"pyTailor Tutorial 4 from pytailor import PythonTask , DAG , Workflow , Project , Outputs import glob import os ### workflow definition ### outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"../*.py\" ], output_to = outputs . parentdir_content , # form 1: single string ) t2 = PythonTask ( function = os . getcwd , name = \"task 2\" , output_extraction = { outputs . curdir : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , outputs . parentdir_content , \"Current working dir:\" , outputs . curdir , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"Example04 outputs parameterization"},{"location":"tutorials/example04_outputs_parameterization/#pytailor-tutorial-4","text":"from pytailor import PythonTask , DAG , Workflow , Project , Outputs import glob import os ### workflow definition ### outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"../*.py\" ], output_to = outputs . parentdir_content , # form 1: single string ) t2 = PythonTask ( function = os . getcwd , name = \"task 2\" , output_extraction = { outputs . curdir : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , outputs . parentdir_content , \"Current working dir:\" , outputs . curdir , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"pyTailor Tutorial 4"},{"location":"tutorials/example05_files/","text":"This example introduces the following NEW concepts: For task definitions: Specifying files to download before running the task Specifying files to upload after task is run Specify which local files to upload when a workflow is created The download argument to task can be a single file tag (str) or a list of file tags. These file tags refer to files in the fileset associated with the workflow. To send input files into a workflow the following steps are taken: instantiate a FileSet object upload files to the fileset with an associated tag pass the fileset along when instantiating the workflow tasks will now download files by referencing the file tags The upload argument to Task is used to specify files to send back to the fileset after a task has been run. upload must be a dict of (tag: val), where val can be: one or more query expressions(str and list of str) which is applied to the function output. The query result is then searched for actual files, these files are then uploaded to storage under the given tag. one or more glob-style strings (str and list of str) which is applied in the task's working dir. Matching files are uploaded under the given tag. File names can be accessed with queries: \"<% $.files.<tag> %>\" which is useful when e.g file name(s) are input to functions. from pytailor import PythonTask , DAG , Workflow , Project , FileSet ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 1\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , # refers to a file tag output_to = \"downloaded_files\" , # put function's return value on $.outputs.downloaded_files ) t2 = PythonTask ( function = \"shutil.copyfile\" , name = \"task 2\" , args = [ \"<% $.files.inpfile[0] %>\" , \"newfile.txt\" ], # inpfile is a tag download = \"inpfile\" , upload = { \"outfile\" : \"newfile.txt\" }, ) t3 = PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"Downloaded\" , \"<% $.files.outfile %>\" ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"Tutorial 5"},{"location":"tutorials/example05_files_parameterization/","text":"pyTailor Tutorial 5 from pytailor import PythonTask , DAG , Workflow , Project , FileSet , Outputs , Files import glob import shutil ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"*.txt\" ], download = files . testfiles , output_to = outputs . downloaded_files , use_storage_dirs = False ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Downloaded\" , files . outfile ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"Example05 files parameterization"},{"location":"tutorials/example05_files_parameterization/#pytailor-tutorial-5","text":"from pytailor import PythonTask , DAG , Workflow , Project , FileSet , Outputs , Files import glob import shutil ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"*.txt\" ], download = files . testfiles , output_to = outputs . downloaded_files , use_storage_dirs = False ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Downloaded\" , files . outfile ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"pyTailor Tutorial 5"},{"location":"tutorials/example06_branch_task/","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching branch_data is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ \"<% $.inputs.data %>\" ]): PythonTask ( function = \"builtins.print\" , name = \"task 1\" , args = [ \"<% $.inputs.data %>\" , \"<% $.inputs.other %>\" ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Tutorial 6"},{"location":"tutorials/example06_branch_task_parameterization/","text":"pyTailor Tutorial 6 This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching nch_data* is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , Inputs ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ inputs . data ]): PythonTask ( function = print , name = \"task 1\" , args = [ inputs . data , inputs . other ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) wf_inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Example06 branch task parameterization"},{"location":"tutorials/example06_branch_task_parameterization/#pytailor-tutorial-6","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching nch_data* is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , Inputs ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ inputs . data ]): PythonTask ( function = print , name = \"task 1\" , args = [ inputs . data , inputs . other ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) wf_inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Tutorial 6"},{"location":"tutorials/example07_branch_dag/","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , FileSet ### workflow definition ### with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) inputs = {} # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , inputs = inputs , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Tutorial 7"},{"location":"tutorials/example07_branch_dag_parameterization/","text":"pyTailor Tutorial 7 This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import ( PythonTask , BranchTask , DAG , Workflow , Project , FileSet , Files , Outputs , ) ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ files . testfiles ], branch_files = [ files . testfiles ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , output_to = outputs . glob_res , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ files . testfiles , outputs . glob_res ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Example07 branch dag parameterization"},{"location":"tutorials/example07_branch_dag_parameterization/#pytailor-tutorial-7","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import ( PythonTask , BranchTask , DAG , Workflow , Project , FileSet , Files , Outputs , ) ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ files . testfiles ], branch_files = [ files . testfiles ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , output_to = outputs . glob_res , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ files . testfiles , outputs . glob_res ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Tutorial 7"},{"location":"tutorials/example08_add_workflow_definition/","text":"This example introduces the following NEW concepts: Create WorkflowDefinition For WorkflowDefinition: use inputsschema to specify the allowed parameters in main DAG use filesschema to specify the allowed files in main DAG use description to describe the workflow definition use account to manage your workflow definitions add the workflow definition to a specific project from pytailor import PythonTask , DAG , WorkflowDefinition , Account , Project , \\ InputsSchema , FilesSchema , Description , Files , Outputs , Inputs , Workflow , \\ FileSet import glob import shutil # a modified dag description with parametrization from example 5 files = Files () outputs = Outputs () inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , # refers to a file tag output_to = outputs . downloaded_files , # put function's return value on # outputs.downloaded_files ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"My input print\" , inputs . print ], parents = t1 , ) # define inputsschema example_inputs = { 'print' : 'hello, world!' } inputs_schema = InputsSchema ( inputs = example_inputs ) inputs_schema . add_defaults ( example_inputs ) # define filesschema files_schema = FilesSchema () files_schema . add_file ( tag = 'inpfile' , ext = [ 'txt' ], required = True , multiple = False ) files_schema . add_file ( tag = 'testfiles' , ext = [ 'txt' ], required = True , multiple = True ) wf_def_description = \"\"\" This workflow definition has the following steps: - Download testfiles and sends filename as output - Download inpfile and upload files - prints the inputs.print argument \"\"\" description = Description . from_dag ( dag , wf_def_name = 'Example 8 Hello world workflow ' 'definition' , wf_def_description = wf_def_description ) # create the workflow definition wf_def = WorkflowDefinition ( name = description . name , description = description . to_string (), dag = dag , inputs_schema = inputs_schema . to_dict (), files_schema = files_schema . to_dict () ) # get an account and add wf_def to account # (requires account admin privileges) account = Account . get_my_accounts ()[ 0 ] wf_def . add_to_account ( account ) # wf_def has now gotten an id print ( wf_def . id ) # the workflow definition can now be added to a project # (requires account admin privileges) prj = Project . from_name ( \"Test\" ) prj . add_workflow_definition ( wf_def . id ) # if you want ... prj . list_available_workflow_definitions () wf_def = WorkflowDefinition . from_project_and_id ( prj , wf_def . id ) fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) wf = Workflow ( project = prj , dag = wf_def . dag , name = \"my workflow\" , inputs = example_inputs , fileset = fileset ) # not yet implemented: # Workflow.from_definition(wf_def.id, # name=\"my workflow\", # inputs=example_inputs, # fileset=fileset) wf . run () # if you want ... prj . remove_workflow_definition ( wf_def . id )","title":"Tutorial 8"},{"location":"tutorials/example09_add_workflow_definition/","text":"This example introduces nested inputs as an example of how a complex input schema is rendered in the web interface from pytailor import PythonTask , BranchTask , DAG , WorkflowDefinition , Account , \\ Project , InputsSchema , Description , Inputs import builtins # a modified dag description with parametrization from example 5 inputs = Inputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( branch_data = inputs . panels , name = 'branch on panels' ) as panel_branch : with BranchTask ( branch_data = inputs . panels . SN_curves , name = 'branch on s/n-curves' ) as sn_curve_branch : t1 = PythonTask ( function = builtins . print , name = \"task 1\" , args = [ inputs . panels . id , inputs . panels . SN_curves ] ) inputs = { 'panels' : [ { 'id' : 'panel1' , 'SN_curves' : [ 1 , 2 , 3 ]}, { 'id' : 'panel2' , 'SN_curves' : [ 1 , 2 ]}, { 'id' : 'panel3' , 'SN_curves' : [ 1 , 2 , 3 , 4 ]}]} # define inputsschema inputs_schema = InputsSchema ( inputs = inputs ) inputs_schema . add_defaults ( inputs ) wf_def_description = \"\"\" This workflow definition has the following steps: - prints the inputs.print argument \"\"\" description = Description . from_dag ( dag , wf_def_name = 'Example 9 Two level branch workflow ' 'definition' , wf_def_description = wf_def_description ) # create the workflow definition wf_def = WorkflowDefinition ( name = description . name , description = description . to_string (), dag = dag , inputs_schema = inputs_schema . to_dict (), ) # get an account and add wf_def to account # (requires account admin privileges) account = Account . get_my_accounts ()[ 0 ] wf_def . add_to_account ( account ) # wf_def has now gotten an id print ( wf_def . id ) # the workflow definition can now be added to a project # (requires account admin privileges) prj = Project . from_name ( \"Test\" ) prj . add_workflow_definition ( wf_def . id )","title":"Tutorial 9"}]}