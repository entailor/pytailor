{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the pyTailor documentation What is pyTailor? pyTailor is a python client for the Tailor automation and collaboration platform. See Tailor.wf for more information. With the pyTailor API you can: Turn your existing python code into parameterized and reusable workflows. Run workflows locally in your computer or distributed to dedicated worker nodes. Store your workflows as workflow definitions in the Tailor backend and share it with others. How does it work? Let's look at a motivational example. Say you have developed a set of python functions to solve some complex computing or engineering problem, and you have organized these functions into a python module called engineering_tasks.py . Say you have a three-step workflow like this: prepare input data for a set of simulations. This is handled by the function engineering_tasks.prepare_simulation_data . Let's say this function: takes a base input file as input. takes a list of parameter dicts for each simulation. produces one simulation input file for each parameter dict. run a set of simulations in parallel. A single simulation is handled by the function engineering_tasks.run_simulation . Let's say this function: takes a .inp file as input produces a .res file as output do post-processing of the simulation data. This is handled by the function engineering_tasks.post_process_simulation_data . Let's say this function: takes a set of .res files as input produces a file report.pdf returns a dict with essential post-processing results With pyTailor you can wrap these functions into PythonTasks , and then use a DAG to define how these tasks relate to each other: from pytailor import PythonTask , BranchTask , DAG , Inputs , Outputs , Files import engineering_tasks inputs = Inputs () outputs = Outputs () files = Files () with DAG ( name = \"Advanced simulation dag\" ) as dag : t1 = PythonTask ( name = \"Pre-processing\" , function = engineering_tasks . prepare_simulation_data , kwargs = { \"parameters\" : inputs . pre_proc_data , \"base_file\" : files . base_file }, download = files . base_file , upload = { files . inp_file : \"sim_inp_file_*.inp\" } ) with BranchTask ( name = \"Parallel simulations\" , branch_files = files . inp_file , parents = t1 ) as branch : PythonTask ( name = \"Simulation\" , function = engineering_tasks . run_simulation , args = [ files . inp_file [ 0 ]], download = files . inp_file , upload = { files . res_file : \"*.res\" } ) PythonTask ( name = \"Post-processing\" , function = engineering_tasks . post_process_simulation_data , args = [ files . res_file ], download = files . res_file , upload = { files . report : \"report.pdf\" }, output_to = outputs . essential_results , parents = branch ) The DAG object represents the recipe for how the the computations shall be performed. By instantiating a DAG no computations are performed, note that we are just referencing the functions we want to use, we are not calling them. DAG is short for Directed Asyclic Graph , a term used to describe the logical flow of computations in a workflow. The DAG defined above is visualized below: A key feature in this DAG is the use of BranchTask to achieve parallelization or \"fan-out\" functionality. The term branching is used to describe this functionality, where one branch is created for each simulation. The Inputs , Outputs and Files objects are helper-objects for parameterization . When we e.g. say kwargs={\"parameters\": inputs.pre_proc_data} we are specifying that the value for the \"parameters\" keyword argument is parameterized and shall be looked up from the pre_proc_data name in the workflow's inputs when the task is executed. The concept of parameterization becomes clearer when we see how inputs , outputs and files are defined when we run a Workflow below. We now have a parameterized DAG describing the recipe of how we want to perform our computing workflow. Based on this definition we can run a Workflow , and we have sereral options: run it directly run it distributed (i.e in parallel, and optionally on several worker machines) Store it as a WorkflowDefinition so that it can be executed directly from the Tailor Webapp. For this example we're just going to run the workflow directly: from pytailor import Project , FileSet , Workflow # open a project prj = Project . from_name ( \"Test\" ) # define inputs workflow_inputs = { \"pre_proc_data\" : [ { \"param1\" : 0 }, { \"param1\" : 1 }, { \"param1\" : 2 }, { \"param1\" : 3 }, { \"param1\" : 4 }, ] } # create a fileset and upload input files fileset = FileSet ( prj ) fileset . upload ( base_file = [ \"testfiles/testfile.inp\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"simulation workflow\" , inputs = workflow_inputs , fileset = fileset ) # run the workflow wf . run () Here we have introduced three new classes from the pyTailor API: Project . A Tailor workflow has to be run in the context of a project. FileSet . Represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run. Workflow . Represents a single workflow run . In order to run a workflow we first instantiate the Workflow object, and then we call the Workflow.run method, which will start executing the workflow in the current python process. Note The direct mode of execution used here is handy when developing new workflows and for testing and debugging of new DAGs. For production workflow runs, distributed mode is suitable. See the worker tutorial for more information. Once the workflow has been started it can be monitored from the Tailor webapp. Below is shown how the workflow can be found in the list of workflows for the specific project by searching for the workflow name. When the worklfow is selected the workflow files appear on the right side for inspection and direct download. Bu clicking on the Details link the workflow can be further inspected in the details view. Get started with pyTailor Head over to the Getting started section for instructions on how to setup pyTailor. Once you are setup and are able to run the basic examples you can start working through the tutorials . You can also consult the API Reference for documentation of the pyTailor API.","title":"Home"},{"location":"#welcome-to-the-pytailor-documentation","text":"","title":"Welcome to the pyTailor documentation"},{"location":"#what-is-pytailor","text":"pyTailor is a python client for the Tailor automation and collaboration platform. See Tailor.wf for more information. With the pyTailor API you can: Turn your existing python code into parameterized and reusable workflows. Run workflows locally in your computer or distributed to dedicated worker nodes. Store your workflows as workflow definitions in the Tailor backend and share it with others.","title":"What is pyTailor?"},{"location":"#how-does-it-work","text":"Let's look at a motivational example. Say you have developed a set of python functions to solve some complex computing or engineering problem, and you have organized these functions into a python module called engineering_tasks.py . Say you have a three-step workflow like this: prepare input data for a set of simulations. This is handled by the function engineering_tasks.prepare_simulation_data . Let's say this function: takes a base input file as input. takes a list of parameter dicts for each simulation. produces one simulation input file for each parameter dict. run a set of simulations in parallel. A single simulation is handled by the function engineering_tasks.run_simulation . Let's say this function: takes a .inp file as input produces a .res file as output do post-processing of the simulation data. This is handled by the function engineering_tasks.post_process_simulation_data . Let's say this function: takes a set of .res files as input produces a file report.pdf returns a dict with essential post-processing results With pyTailor you can wrap these functions into PythonTasks , and then use a DAG to define how these tasks relate to each other: from pytailor import PythonTask , BranchTask , DAG , Inputs , Outputs , Files import engineering_tasks inputs = Inputs () outputs = Outputs () files = Files () with DAG ( name = \"Advanced simulation dag\" ) as dag : t1 = PythonTask ( name = \"Pre-processing\" , function = engineering_tasks . prepare_simulation_data , kwargs = { \"parameters\" : inputs . pre_proc_data , \"base_file\" : files . base_file }, download = files . base_file , upload = { files . inp_file : \"sim_inp_file_*.inp\" } ) with BranchTask ( name = \"Parallel simulations\" , branch_files = files . inp_file , parents = t1 ) as branch : PythonTask ( name = \"Simulation\" , function = engineering_tasks . run_simulation , args = [ files . inp_file [ 0 ]], download = files . inp_file , upload = { files . res_file : \"*.res\" } ) PythonTask ( name = \"Post-processing\" , function = engineering_tasks . post_process_simulation_data , args = [ files . res_file ], download = files . res_file , upload = { files . report : \"report.pdf\" }, output_to = outputs . essential_results , parents = branch ) The DAG object represents the recipe for how the the computations shall be performed. By instantiating a DAG no computations are performed, note that we are just referencing the functions we want to use, we are not calling them. DAG is short for Directed Asyclic Graph , a term used to describe the logical flow of computations in a workflow. The DAG defined above is visualized below: A key feature in this DAG is the use of BranchTask to achieve parallelization or \"fan-out\" functionality. The term branching is used to describe this functionality, where one branch is created for each simulation. The Inputs , Outputs and Files objects are helper-objects for parameterization . When we e.g. say kwargs={\"parameters\": inputs.pre_proc_data} we are specifying that the value for the \"parameters\" keyword argument is parameterized and shall be looked up from the pre_proc_data name in the workflow's inputs when the task is executed. The concept of parameterization becomes clearer when we see how inputs , outputs and files are defined when we run a Workflow below. We now have a parameterized DAG describing the recipe of how we want to perform our computing workflow. Based on this definition we can run a Workflow , and we have sereral options: run it directly run it distributed (i.e in parallel, and optionally on several worker machines) Store it as a WorkflowDefinition so that it can be executed directly from the Tailor Webapp. For this example we're just going to run the workflow directly: from pytailor import Project , FileSet , Workflow # open a project prj = Project . from_name ( \"Test\" ) # define inputs workflow_inputs = { \"pre_proc_data\" : [ { \"param1\" : 0 }, { \"param1\" : 1 }, { \"param1\" : 2 }, { \"param1\" : 3 }, { \"param1\" : 4 }, ] } # create a fileset and upload input files fileset = FileSet ( prj ) fileset . upload ( base_file = [ \"testfiles/testfile.inp\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"simulation workflow\" , inputs = workflow_inputs , fileset = fileset ) # run the workflow wf . run () Here we have introduced three new classes from the pyTailor API: Project . A Tailor workflow has to be run in the context of a project. FileSet . Represents an isolated file storage area in the Tailor backend and is associated with a specific workflow run. Workflow . Represents a single workflow run . In order to run a workflow we first instantiate the Workflow object, and then we call the Workflow.run method, which will start executing the workflow in the current python process. Note The direct mode of execution used here is handy when developing new workflows and for testing and debugging of new DAGs. For production workflow runs, distributed mode is suitable. See the worker tutorial for more information. Once the workflow has been started it can be monitored from the Tailor webapp. Below is shown how the workflow can be found in the list of workflows for the specific project by searching for the workflow name. When the worklfow is selected the workflow files appear on the right side for inspection and direct download. Bu clicking on the Details link the workflow can be further inspected in the details view.","title":"How does it work?"},{"location":"#get-started-with-pytailor","text":"Head over to the Getting started section for instructions on how to setup pyTailor. Once you are setup and are able to run the basic examples you can start working through the tutorials . You can also consult the API Reference for documentation of the pyTailor API.","title":"Get started with pyTailor"},{"location":"api/parameterization/","text":"Parameterization Classes Inputs class pytailor. Inputs ( ) Helper object for inputs parameterization. Basic usage from pytailor import PythonTask , DAG , Inputs , Workflow , Project inputs = Inputs () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = inputs . hello , ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , inputs = { \"hello\" : [ \"Hello, world!\" ]} ) wf . run () Outputs class pytailor. Outputs ( ) Helper object for outputs parameterization. Basic usage from pytailor import PythonTask , DAG , Outputs , Workflow , Project outputs = Outputs () with DAG () as dag : t1 = PythonTask ( function = 'os.getcwd' , output_to = outputs . curdir ) PythonTask ( function = 'builtins.print' , args = [ outputs . curdir ], ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , ) wf . run () print ( wf . outputs ) Files class pytailor. Files ( ) Helper object for files parameterization. Basic usage from pytailor import PythonTask , DAG , Files , Workflow , Project , FileSet files = Files () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = [ \"This tasks download the file:\" , files . inpfile [ 0 ]], download = files . inpfile ) prj = Project . from_name ( \"Test\" ) fileset = FileSet ( project = prj ) fileset . upload ( inpfile = [ \"my_file.txt\" ]) wf = Workflow ( dag = dag , project = prj , fileset = fileset ) wf . run ()","title":"Parameterization classes"},{"location":"api/parameterization/#parameterization-classes","text":"","title":"Parameterization Classes"},{"location":"api/parameterization/#inputs","text":"class pytailor. Inputs ( ) Helper object for inputs parameterization. Basic usage from pytailor import PythonTask , DAG , Inputs , Workflow , Project inputs = Inputs () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = inputs . hello , ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , inputs = { \"hello\" : [ \"Hello, world!\" ]} ) wf . run ()","title":"Inputs"},{"location":"api/parameterization/#outputs","text":"class pytailor. Outputs ( ) Helper object for outputs parameterization. Basic usage from pytailor import PythonTask , DAG , Outputs , Workflow , Project outputs = Outputs () with DAG () as dag : t1 = PythonTask ( function = 'os.getcwd' , output_to = outputs . curdir ) PythonTask ( function = 'builtins.print' , args = [ outputs . curdir ], ) prj = Project . from_name ( \"Test\" ) wf = Workflow ( dag = dag , project = prj , ) wf . run () print ( wf . outputs )","title":"Outputs"},{"location":"api/parameterization/#files","text":"class pytailor. Files ( ) Helper object for files parameterization. Basic usage from pytailor import PythonTask , DAG , Files , Workflow , Project , FileSet files = Files () with DAG () as dag : PythonTask ( function = 'builtins.print' , args = [ \"This tasks download the file:\" , files . inpfile [ 0 ]], download = files . inpfile ) prj = Project . from_name ( \"Test\" ) fileset = FileSet ( project = prj ) fileset . upload ( inpfile = [ \"my_file.txt\" ]) wf = Workflow ( dag = dag , project = prj , fileset = fileset ) wf . run ()","title":"Files"},{"location":"api/schema/","text":"Schema Definition Classes InputsSchema class pytailor. InputsSchema ( inputs ) Generator for inputsschema to define workflow definition Basic usage Following example define a jsonschema, that sets \"print\" as a required property for inputs and that the value must be a string example_inputs = { 'print' : 'Hello, world!' } inputsschema = InputsSchema ( inputs = example_inputs ) Add 'Hello, world' as a default for property 'print' example_inputs = { 'print' : 'Hello, world!' } inputsschema . add_defaults ( example_inputs ) # Set 'Hello, world' and 'Hello, tailor!' as only allowed alternatives for property 'print': enum_inputs = { 'print' : [ 'Hello, world!' , 'Hello, tailor!' ]} inputsschema . add_enums ( enum_inputs ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_defaults ( self , default_inputs ) Parameters default_inputs (dict) S Specify an example input schema that is valid for your workflow definition with defaults. add_enums ( self , enum_inputs ) Parameters enum_inputs (dict) S Specify altenatives for your inputsschema with a input schema where the property's values in a list represents the alternatives to_dict ( self ) Serialize input schema to_json ( self , filename , indent=4 ) a json file FilesSchema class pytailor. FilesSchema ( tags=None , exts=None , multiples=None , requireds=None , titles=None , descriptions=None ) Generator for filesschema to define workflow definition Basic usage files_ex1 = { \"tag\" : \"my_tag\" , \"ext\" : [ \"txt\" ], } filesschema = FilesSchema () filesschema . add_file ( tag = \"my_tag\" , ext = [ \"txt\" ]) files_ex2 = { \"tag\" : \"my__new_tag\" , \"ext\" : [ \"txt\" ], \"title\" : \"Coordinates\" , \"multiple\" : True , \"description\" : \"A file with coordinate values of nodes\" , \"required\" : True , } filesschema = FilesSchema () filesschema . add_file ( ** files_ex2 ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_file ( self , tag , ext=None , multiple=False , required=True , title=None , description='' ) Parameters tag (str) Specify tag for file. tag (str) Specify allowed extension for file mutiple (bool) Specify whether multiples files are allowed required (bool) Specify whether file(s) are required title (str) Specify the title to be shown in GUI description (str) Specify the description to be shown in GUI to_dict ( self ) Serialize files schema to_json ( self , filename , indent=4 ) Description class pytailor. Description ( name=None , description_string=None ) Workflow definition description generator to define workflow definition Basic usage wf_def_description = \"This example explains branchtasks\" wf_def_name = \"branch task example\" from pytailor import PythonTask , BranchTask , DAG with DAG ( name = \"duplicate dag example\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"* / .txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) description = Description . from_dag ( dag , wf_def_name = wf_def_name , wf_def_description = wf_def_description ) Parameters name (dict) Specify a name for the workflow definition description_string (str) Specify a description string in markdown format from_dag ( dag , wf_def_name='' , wf_def_description='' ) Generates a workflow definition description from a pytailor DAG class Parameters dag (DAG) Specify your DAG for workflow definition wf_def_name (str) Specify a name for workflow definition wf_def_description (str) Specify an overall description, what are the main objectives of your workflow definition? What are the main steps in the DAG? to_string ( self ) to_markdown ( self , filename='Readme.MD' )","title":"Schema definition classes"},{"location":"api/schema/#schema-definition-classes","text":"","title":"Schema Definition Classes"},{"location":"api/schema/#inputsschema","text":"class pytailor. InputsSchema ( inputs ) Generator for inputsschema to define workflow definition Basic usage Following example define a jsonschema, that sets \"print\" as a required property for inputs and that the value must be a string example_inputs = { 'print' : 'Hello, world!' } inputsschema = InputsSchema ( inputs = example_inputs ) Add 'Hello, world' as a default for property 'print' example_inputs = { 'print' : 'Hello, world!' } inputsschema . add_defaults ( example_inputs ) # Set 'Hello, world' and 'Hello, tailor!' as only allowed alternatives for property 'print': enum_inputs = { 'print' : [ 'Hello, world!' , 'Hello, tailor!' ]} inputsschema . add_enums ( enum_inputs ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_defaults ( self , default_inputs ) Parameters default_inputs (dict) S Specify an example input schema that is valid for your workflow definition with defaults. add_enums ( self , enum_inputs ) Parameters enum_inputs (dict) S Specify altenatives for your inputsschema with a input schema where the property's values in a list represents the alternatives to_dict ( self ) Serialize input schema to_json ( self , filename , indent=4 ) a json file","title":"InputsSchema"},{"location":"api/schema/#filesschema","text":"class pytailor. FilesSchema ( tags=None , exts=None , multiples=None , requireds=None , titles=None , descriptions=None ) Generator for filesschema to define workflow definition Basic usage files_ex1 = { \"tag\" : \"my_tag\" , \"ext\" : [ \"txt\" ], } filesschema = FilesSchema () filesschema . add_file ( tag = \"my_tag\" , ext = [ \"txt\" ]) files_ex2 = { \"tag\" : \"my__new_tag\" , \"ext\" : [ \"txt\" ], \"title\" : \"Coordinates\" , \"multiple\" : True , \"description\" : \"A file with coordinate values of nodes\" , \"required\" : True , } filesschema = FilesSchema () filesschema . add_file ( ** files_ex2 ) Parameters inputs (dict) Specify an example input schema that is valid for your workflow definition. add_file ( self , tag , ext=None , multiple=False , required=True , title=None , description='' ) Parameters tag (str) Specify tag for file. tag (str) Specify allowed extension for file mutiple (bool) Specify whether multiples files are allowed required (bool) Specify whether file(s) are required title (str) Specify the title to be shown in GUI description (str) Specify the description to be shown in GUI to_dict ( self ) Serialize files schema to_json ( self , filename , indent=4 )","title":"FilesSchema"},{"location":"api/schema/#description","text":"class pytailor. Description ( name=None , description_string=None ) Workflow definition description generator to define workflow definition Basic usage wf_def_description = \"This example explains branchtasks\" wf_def_name = \"branch task example\" from pytailor import PythonTask , BranchTask , DAG with DAG ( name = \"duplicate dag example\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"* / .txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) description = Description . from_dag ( dag , wf_def_name = wf_def_name , wf_def_description = wf_def_description ) Parameters name (dict) Specify a name for the workflow definition description_string (str) Specify a description string in markdown format from_dag ( dag , wf_def_name='' , wf_def_description='' ) Generates a workflow definition description from a pytailor DAG class Parameters dag (DAG) Specify your DAG for workflow definition wf_def_name (str) Specify a name for workflow definition wf_def_description (str) Specify an overall description, what are the main objectives of your workflow definition? What are the main steps in the DAG? to_string ( self ) to_markdown ( self , filename='Readme.MD' )","title":"Description"},{"location":"api/taskdefs/","text":"Task definition classes PythonTask class pytailor. PythonTask ( function , name=None , parents=None , owner=None , download=None , upload=None , args=None , kwargs=None , output_to=None , output_extraction=None , use_storage_dirs=True , requirements=None ) Task for running python code. Basic usage pytask = PythonTask ( function = 'builtins.print' , args = 'Hello, world!' , name = 'My first task' ) Parameters function (str or Callable) Python callable. Must be importable in the executing python env. name (str, optional) A default name is used if not provided. parents (BaseTask or List[BaseTask], optional) Specify one or more upstream tasks that this task depends on. download (str, list or Parameterization, optional) Provide one or more file tags. These file tags refer to files in the storage object associated with the workflow run. upload (dict, optional) Specify files to send back to the storage object after a task has been run. Dict format is {tag1: val1, tag2: val2, ...} where val can be: one or more query expressions(str og list) which is applied to the return value from callable . File names resulting from the query are then uploaded to storage under the given tag. one or more glob-style strings (str og list) which is applied in the task working dir. matching files are uploaded under the given tag. args (list, str or Parameterization, optional) Arguments to be passed as positional arguments to function . Accepts a list of ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a list. See the examples for how parameterization objects and query expressions are used. kwargs (dict, str or Parameterization, optional) Arguments to be passed as keyword arguments to function . accepts a kwargs dict where values can be ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a dict. See the examples for how parameterization objects and query expressions are used. output_to (str or Parameterization, optional) The return value of the callable is stored under the provided name in the workflow outputs . This value is then available for downstream task. output_extraction (dict, optional) Provide a dict of name: expr where expr are query-expressions to extract parts of the return value of the callable. The keys of the dict are used as names for storing in the workflow outputs which becomes available for downstream tasks. to_dict ( self ) Serialize task definition. from_dict ( d ) Create from serialized task definition. DAG class pytailor. DAG ( tasks=None , name=None , parents=None , owner=None , links=None , requirements=None ) Represents a Directed Acyclic Graph, i.e. a DAG. Parameters tasks : BaseTask or List[BaseTask] Python, Duplicate or WorkflowSpec objects. name : str, optional A default name is used if not provided. parents : BaseTask or List[BaseTask], optional Specify one or more upstream tasks that this task depends on. links : dict, optional Parent/children relationships can be specified with the dict on the form {parent_def: [child_def1, child_def2], ...}. Definition references may either be indices (ints) into tasks or BaseTask instances. Note that links may also be defined on task objects with the parents argument instead of using links: (parents=[parent_def1, parent_def2]) to_dict ( self ) from_dict ( d ) BranchTask class pytailor. BranchTask ( task=None , name=None , parents=None , owner=None , branch_data=None , branch_files=None , requirements=None ) Dynamically branch a task or DAG during workflow execution. BranchTask Provides parallelization or \"fan-out\" functionality. The task is duplicated based on branch_data or branch_files . At least one of these must be specified. Parameters task : BaseTask Task to be duplicated (PythonTask, BranchTask or DAG). name : str, optional A default name is used if not provided. parents : BaseTask or List[BaseTask], optional Specify one or more upstream tasks that this task depends on. branch_data : list or str, optional Data to be used as basis for branching. Accepts a query-expression or a list of query-expressions. The queries must evaluate to a list or a dict. If the query evaluates to a dict, that dict must have integer keys to represent the index of each branch. branch_files : list or str, optional Files to be used as basis for branching. Accepts a file tag or a list of file tags. to_dict ( self ) from_dict ( d )","title":"Task definition classes"},{"location":"api/taskdefs/#task-definition-classes","text":"","title":"Task definition classes"},{"location":"api/taskdefs/#pythontask","text":"class pytailor. PythonTask ( function , name=None , parents=None , owner=None , download=None , upload=None , args=None , kwargs=None , output_to=None , output_extraction=None , use_storage_dirs=True , requirements=None ) Task for running python code. Basic usage pytask = PythonTask ( function = 'builtins.print' , args = 'Hello, world!' , name = 'My first task' ) Parameters function (str or Callable) Python callable. Must be importable in the executing python env. name (str, optional) A default name is used if not provided. parents (BaseTask or List[BaseTask], optional) Specify one or more upstream tasks that this task depends on. download (str, list or Parameterization, optional) Provide one or more file tags. These file tags refer to files in the storage object associated with the workflow run. upload (dict, optional) Specify files to send back to the storage object after a task has been run. Dict format is {tag1: val1, tag2: val2, ...} where val can be: one or more query expressions(str og list) which is applied to the return value from callable . File names resulting from the query are then uploaded to storage under the given tag. one or more glob-style strings (str og list) which is applied in the task working dir. matching files are uploaded under the given tag. args (list, str or Parameterization, optional) Arguments to be passed as positional arguments to function . Accepts a list of ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a list. See the examples for how parameterization objects and query expressions are used. kwargs (dict, str or Parameterization, optional) Arguments to be passed as keyword arguments to function . accepts a kwargs dict where values can be ordinary python values, parameterization objects or query expressions. Also accepts a single single parameterization object or query expression which evaluate to a dict. See the examples for how parameterization objects and query expressions are used. output_to (str or Parameterization, optional) The return value of the callable is stored under the provided name in the workflow outputs . This value is then available for downstream task. output_extraction (dict, optional) Provide a dict of name: expr where expr are query-expressions to extract parts of the return value of the callable. The keys of the dict are used as names for storing in the workflow outputs which becomes available for downstream tasks. to_dict ( self ) Serialize task definition. from_dict ( d ) Create from serialized task definition.","title":"PythonTask"},{"location":"api/taskdefs/#dag","text":"class pytailor. DAG ( tasks=None , name=None , parents=None , owner=None , links=None , requirements=None ) Represents a Directed Acyclic Graph, i.e. a DAG.","title":"DAG"},{"location":"api/taskdefs/#branchtask","text":"class pytailor. BranchTask ( task=None , name=None , parents=None , owner=None , branch_data=None , branch_files=None , requirements=None ) Dynamically branch a task or DAG during workflow execution. BranchTask Provides parallelization or \"fan-out\" functionality. The task is duplicated based on branch_data or branch_files . At least one of these must be specified.","title":"BranchTask"},{"location":"documentation/concepts/","text":"Tailor concepts DAGs DAGs are used to define relationships between tasks in a Workflow in the form of a Directed Acyclic Graph. Task definitions Task definitions are parameterized and reusable blueprints for computing tasks. In Pytailor, tasks definitions are created using the task definition classes The available task definition classes are: PythonTask This is the basic building block. Used to define the execution of a single Python function (callable). BranchTask ... By combining the different task types, arbitrary complex tasks can be defined. A non-trivial task definition will typically consist of a DAG at the top-level, which in turn consist of other task definitions as illustrated in ... Note Tailor is still in development and more task definition classes are likely to be introduced in the future to extend functionality. Workflows Workflows are instantiated DAGs with a given set of inputs and files. Workflows are stored on the Tailor backend under a given Project . dag project inputs (parameters, JSON) files (inputs files) worker requirements In Pytailor, workflows are represented by the Workflow class Tasks Tasks are instantiated task definitions . Belongs to a Workflow . Workflow definitions Workflow definitions are used to store a DAG along with requirements (schema) for inputs and files. Workflow definitions are stored in the backend and can be made available for other users. Workflow subscriptions Workflow definitions can be shared between users with Workflow subscriptions . Tailor Accounts Tailor Projects","title":"Tailor concepts"},{"location":"documentation/concepts/#tailor-concepts","text":"","title":"Tailor concepts"},{"location":"documentation/concepts/#dags","text":"DAGs are used to define relationships between tasks in a Workflow in the form of a Directed Acyclic Graph.","title":"DAGs"},{"location":"documentation/concepts/#task-definitions","text":"Task definitions are parameterized and reusable blueprints for computing tasks. In Pytailor, tasks definitions are created using the task definition classes The available task definition classes are: PythonTask This is the basic building block. Used to define the execution of a single Python function (callable). BranchTask ... By combining the different task types, arbitrary complex tasks can be defined. A non-trivial task definition will typically consist of a DAG at the top-level, which in turn consist of other task definitions as illustrated in ... Note Tailor is still in development and more task definition classes are likely to be introduced in the future to extend functionality.","title":"Task definitions"},{"location":"documentation/concepts/#workflows","text":"Workflows are instantiated DAGs with a given set of inputs and files. Workflows are stored on the Tailor backend under a given Project . dag project inputs (parameters, JSON) files (inputs files) worker requirements In Pytailor, workflows are represented by the Workflow class","title":"Workflows"},{"location":"documentation/concepts/#tasks","text":"Tasks are instantiated task definitions . Belongs to a Workflow .","title":"Tasks"},{"location":"documentation/concepts/#workflow-definitions","text":"Workflow definitions are used to store a DAG along with requirements (schema) for inputs and files. Workflow definitions are stored in the backend and can be made available for other users.","title":"Workflow definitions"},{"location":"documentation/concepts/#workflow-subscriptions","text":"Workflow definitions can be shared between users with Workflow subscriptions .","title":"Workflow subscriptions"},{"location":"documentation/concepts/#tailor-accounts","text":"","title":"Tailor Accounts"},{"location":"documentation/concepts/#tailor-projects","text":"","title":"Tailor Projects"},{"location":"documentation/contexts/","text":"Contexts Contexts represents the data and files associated with a workflow. The context consist of three datastructures: inputs outputs files inputs are provided by the user during workflow specification, e.g. from tailor import PythonTask , WorkflowSpec # a task definition t1 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) t2 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) inputs = { input_number : - 123 } wf_spec = WorkflowSpec ( ) Note The inputs and outputs datastructures must be JSON-serializable, which limits the data types which can be used. In the future more sophisticated serialization may be applied to allow other object types, e.g. numpy arrays. For data that is not JSON-compatible you can serialize the data to file and use the file-piping mechanisms to send the data to your tasks. Context queries Contexts can be queried using the YAQL query language. Context queries can be used in task definitions as a means to parameterize inputs. When jobs are executed, the queries are performed on the context associated with the current workflow run. Context queries, when used in task definitions, are specified using a special syntax: '<% query-expression %>' . This syntax tells tailor to Scoped contexts Scoped contexts arise when tasks are duplicated. Consider the following DAG, consisting of two PythonTasks: +---+ | t1 | PythonTask ( ... , output_to = 'out' ) +-+-+ | | +- v -+ | t2 | PythonTask ( ... , args = '<% $.outputs.out %>' ) +---+ When this DAG is duplicated with a BranchTask","title":"Contexts"},{"location":"documentation/contexts/#contexts","text":"Contexts represents the data and files associated with a workflow. The context consist of three datastructures: inputs outputs files inputs are provided by the user during workflow specification, e.g. from tailor import PythonTask , WorkflowSpec # a task definition t1 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) t2 = Pythontask ( function = 'builtins.abs' , args = '<% $.inputs.input_number %>' ) inputs = { input_number : - 123 } wf_spec = WorkflowSpec ( ) Note The inputs and outputs datastructures must be JSON-serializable, which limits the data types which can be used. In the future more sophisticated serialization may be applied to allow other object types, e.g. numpy arrays. For data that is not JSON-compatible you can serialize the data to file and use the file-piping mechanisms to send the data to your tasks.","title":"Contexts"},{"location":"documentation/contexts/#context-queries","text":"Contexts can be queried using the YAQL query language. Context queries can be used in task definitions as a means to parameterize inputs. When jobs are executed, the queries are performed on the context associated with the current workflow run. Context queries, when used in task definitions, are specified using a special syntax: '<% query-expression %>' . This syntax tells tailor to","title":"Context queries"},{"location":"documentation/contexts/#scoped-contexts","text":"Scoped contexts arise when tasks are duplicated. Consider the following DAG, consisting of two PythonTasks: +---+ | t1 | PythonTask ( ... , output_to = 'out' ) +-+-+ | | +- v -+ | t2 | PythonTask ( ... , args = '<% $.outputs.out %>' ) +---+ When this DAG is duplicated with a BranchTask","title":"Scoped contexts"},{"location":"documentation/getting_started/","text":"Getting started To use Pytailor you need to connect to a Tailor backend. The easiest way to get started with Tailor is to sign up for a free account at Tailor.wf . For other options please contact us . In the following it us assumed that you are using Tailor.wf as your backend service. Installation First time setup 1. Install pytailor Official releases of pytailor are available on pypi and can easily be installed with pip: pip install pytailor Note You can also clone the repository on github and install pytailor with poetry : git clone https://github.com/entailor/pytailor.git Then, from the project root run: poetry install To install without development dependencies: poetry install --no-dev 2. Configure backend When pytailor is installed your ca use the CLI to set up a barebone config file: tailor init This command generates a config file .tailor/config.toml under your home directory with the following content: API_KEY = < API KEY GOES HERE > API_BASE_URL = < URL FOR THE BACKEND REST API GOES HERE > Note It is also possible to configure pytailor with environmental variables by prefixing the environmental variables with PYTAILOR_ . E.g. to set the API, key put it in an environmental variable called PYTAILOR_API_KEY . Update Pytailor Basic usage With Pytailor installed and a backend properly configured you should be able to run the following example: from tailor import PythonTask , DAG In this example, several key concepts are illustrated: ... walk through code... Please consult the Consepts page...","title":"Getting started"},{"location":"documentation/getting_started/#getting-started","text":"To use Pytailor you need to connect to a Tailor backend. The easiest way to get started with Tailor is to sign up for a free account at Tailor.wf . For other options please contact us . In the following it us assumed that you are using Tailor.wf as your backend service.","title":"Getting started"},{"location":"documentation/getting_started/#installation","text":"","title":"Installation"},{"location":"documentation/getting_started/#first-time-setup","text":"","title":"First time setup"},{"location":"documentation/getting_started/#1-install-pytailor","text":"Official releases of pytailor are available on pypi and can easily be installed with pip: pip install pytailor Note You can also clone the repository on github and install pytailor with poetry : git clone https://github.com/entailor/pytailor.git Then, from the project root run: poetry install To install without development dependencies: poetry install --no-dev","title":"1. Install pytailor"},{"location":"documentation/getting_started/#2-configure-backend","text":"When pytailor is installed your ca use the CLI to set up a barebone config file: tailor init This command generates a config file .tailor/config.toml under your home directory with the following content: API_KEY = < API KEY GOES HERE > API_BASE_URL = < URL FOR THE BACKEND REST API GOES HERE > Note It is also possible to configure pytailor with environmental variables by prefixing the environmental variables with PYTAILOR_ . E.g. to set the API, key put it in an environmental variable called PYTAILOR_API_KEY .","title":"2. Configure backend"},{"location":"documentation/getting_started/#update-pytailor","text":"","title":"Update Pytailor"},{"location":"documentation/getting_started/#basic-usage","text":"With Pytailor installed and a backend properly configured you should be able to run the following example: from tailor import PythonTask , DAG In this example, several key concepts are illustrated: ... walk through code... Please consult the Consepts page...","title":"Basic usage"},{"location":"tutorials/example01_hello_world/","text":"pyTailor Example 1 This is the Hello world example for pyTailor. This example introduces the following NEW concepts: Create PythonTasks and DAGs For PythonTasks: Specifying the function to run (must be an importable python function) Specifying a name for the task Specifying positional arguments (*args) to the function Specifying relationships between tasks For DAGs: Specifying which tasks are part of the DAG Specifying a name for the DAG Create a Workflow and run it in 'here_and_now' mode Check status of the resulting Workflow after execution Retrieve a workflow from the backend into a new Workflow object from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) dag = DAG ( tasks = [ t1 , t2 ], name = \"dag\" ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"Tutorial 1"},{"location":"tutorials/example01_hello_world/#pytailor-example-1","text":"This is the Hello world example for pyTailor. This example introduces the following NEW concepts: Create PythonTasks and DAGs For PythonTasks: Specifying the function to run (must be an importable python function) Specifying a name for the task Specifying positional arguments (*args) to the function Specifying relationships between tasks For DAGs: Specifying which tasks are part of the DAG Specifying a name for the DAG Create a Workflow and run it in 'here_and_now' mode Check status of the resulting Workflow after execution Retrieve a workflow from the backend into a new Workflow object from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) dag = DAG ( tasks = [ t1 , t2 ], name = \"dag\" ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"pyTailor Example 1"},{"location":"tutorials/example01_hello_world_alt_syntax1/","text":"pyTailor Example 1, alternative syntax 1 Here is the Hello world example with alternative syntax allowing for definition of the DAG in \"reversed\" order by using the owner parameter. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### dag = DAG ( name = \"dag\" ) t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], owner = dag , ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , owner = dag , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"Example01 hello world alt syntax1"},{"location":"tutorials/example01_hello_world_alt_syntax1/#pytailor-example-1-alternative-syntax-1","text":"Here is the Hello world example with alternative syntax allowing for definition of the DAG in \"reversed\" order by using the owner parameter. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### dag = DAG ( name = \"dag\" ) t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], owner = dag , ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , owner = dag , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"pyTailor Example 1, alternative syntax 1"},{"location":"tutorials/example01_hello_world_alt_syntax2/","text":"pyTailor Example 1, alternative syntax 2 Here is the Hello world example with alternative syntax using context managers. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"Example01 hello world alt syntax2"},{"location":"tutorials/example01_hello_world_alt_syntax2/#pytailor-example-1-alternative-syntax-2","text":"Here is the Hello world example with alternative syntax using context managers. from pytailor import PythonTask , DAG , Project , Workflow ### dag definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"builtins.print\" , # function='builtins.abs', # will raise type error name = \"job 1\" , args = [ \" \\n Hello, world! \\n \" ], ) t2 = PythonTask ( function = \"builtins.print\" , name = \"job 2\" , args = [ \" \\n Hello again,\" , \"world! \\n \" ], parents = t1 , ) # open a project prj = Project . from_name ( \"Test\" ) ### workflow execution ### # create a workflow wf = Workflow ( project = prj , dag = dag , name = \"Hello from Pytailor\" , ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state ) ### workflow retrieval ### wf2 = Workflow . from_project_and_id ( prj , wf . id ) assert wf . id == wf2 . id # pretty print the workflow print ( wf )","title":"pyTailor Example 1, alternative syntax 2"},{"location":"tutorials/example02_kwargs/","text":"pyTailor Example 2 This example introduces the following NEW concepts: For PythonTasks: Specifying keyword arguments (**kwargs) to the function from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"time.sleep\" , name = \"task 1\" , args = [ 1 ]) t2 = PythonTask ( function = \"builtins.print\" , name = \"task 2\" , args = [ \" \\n Slept for\" , \"1\" , \"second\" ], kwargs = { \"sep\" : \" \" , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"kwarg workflow\" ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Tutorial 2"},{"location":"tutorials/example02_kwargs/#pytailor-example-2","text":"This example introduces the following NEW concepts: For PythonTasks: Specifying keyword arguments (**kwargs) to the function from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"time.sleep\" , name = \"task 1\" , args = [ 1 ]) t2 = PythonTask ( function = \"builtins.print\" , name = \"task 2\" , args = [ \" \\n Slept for\" , \"1\" , \"second\" ], kwargs = { \"sep\" : \" \" , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"kwarg workflow\" ) # run the workflow wf . run () # check the status of the workflow run print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Example 2"},{"location":"tutorials/example03_inputs/","text":"pyTailor Example 3 This example introduces the following NEW concepts: For task definitions: Using query expressions. Specifying inputs when creating a workflow Query expressions is a means to parameterize the inputs that are specified in a task definition. The query will be applied later when the workflow is executed and the parameters to use will be extracted from the input provided to that specific workflow. To use a query, the query string must be on the format \"<% query %>\". The yaql python package is used for handling queries, see https://yaql.readthedocs.io/en/latest/index.html. Inputs are immutable, in the sense that they cannot be changed during the execution of the workflow. NOTE: Currently this mechanism only work with data that is directly -serializable. In the future non-JSON compatible objects may be handled as well (by use of pickling). from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"time.sleep\" , name = \"task 1\" , args = [ \"<% $.inputs.sleep_time %>\" ] ) t2 = PythonTask ( function = \"builtins.print\" , name = \"task 2\" , args = [ \" \\n Slept for\" , \"<% $.inputs.sleep_time %>\" , \"second\" ], kwargs = { \"sep\" : \" \" , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs inputs = { \"sleep_time\" : 1.5 } # try to change this and rerun the workflow # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"Tutorial 3"},{"location":"tutorials/example03_inputs/#pytailor-example-3","text":"This example introduces the following NEW concepts: For task definitions: Using query expressions. Specifying inputs when creating a workflow Query expressions is a means to parameterize the inputs that are specified in a task definition. The query will be applied later when the workflow is executed and the parameters to use will be extracted from the input provided to that specific workflow. To use a query, the query string must be on the format \"<% query %>\". The yaql python package is used for handling queries, see https://yaql.readthedocs.io/en/latest/index.html. Inputs are immutable, in the sense that they cannot be changed during the execution of the workflow. NOTE: Currently this mechanism only work with data that is directly -serializable. In the future non-JSON compatible objects may be handled as well (by use of pickling). from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"time.sleep\" , name = \"task 1\" , args = [ \"<% $.inputs.sleep_time %>\" ] ) t2 = PythonTask ( function = \"builtins.print\" , name = \"task 2\" , args = [ \" \\n Slept for\" , \"<% $.inputs.sleep_time %>\" , \"second\" ], kwargs = { \"sep\" : \" \" , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs inputs = { \"sleep_time\" : 1.5 } # try to change this and rerun the workflow # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"pyTailor Example 3"},{"location":"tutorials/example03_inputs_parameterization/","text":"pyTailor Example 3 from pytailor import PythonTask , DAG , Workflow , Project , Inputs import time ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = time . sleep , name = \"task 1\" , args = [ inputs . data . sleep_time [ 0 ]] ) t2 = PythonTask ( function = print , name = \"task 2\" , args = [ \" \\n Slept for\" , inputs . data , \"second\" ], kwargs = { \"sep\" : inputs . sep , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs wf_inputs = { \"data\" : { \"sleep_time\" : [ 1.5 ]}, \"sep\" : \" \" } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"Example03 inputs parameterization"},{"location":"tutorials/example03_inputs_parameterization/#pytailor-example-3","text":"from pytailor import PythonTask , DAG , Workflow , Project , Inputs import time ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = time . sleep , name = \"task 1\" , args = [ inputs . data . sleep_time [ 0 ]] ) t2 = PythonTask ( function = print , name = \"task 2\" , args = [ \" \\n Slept for\" , inputs . data , \"second\" ], kwargs = { \"sep\" : inputs . sep , \"end\" : \" \\n\\n \" }, parents = t1 , ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # define inputs wf_inputs = { \"data\" : { \"sleep_time\" : [ 1.5 ]}, \"sep\" : \" \" } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"inputs workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # inputs are available on the run object print ( \"Inputs were:\" ) print ( wf . inputs )","title":"pyTailor Example 3"},{"location":"tutorials/example04_outputs/","text":"pyTailor Example 4 This example introduces the following NEW concepts: For PythonTask definitions: Specifying what to do with function output Accessing function output in downstream tasks Specifying multiple parents The output argument to PythonTask can be specified in two forms: 1. A single string. Then the entire return value of the function is put on $.outputs. . 2. A dictionary. Then for each (tag: query) in the dict the query is applied to the return value and the result is put on $.outputs. The output can be accessed in downstream tasks using query expressions like \"<% $.outputs. %>\". The output is also available as an attribute (dict) on the workflow run objects retrieved from the database (WorkflowRun.outputs). NOTE: Currently this mechanism only work with data that is directly -serializable. In the future non-JSON compatible objects will be handled as well (by use of pickling). from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 1\" , args = [ \"../*.py\" ], output_to = \"parentdir_content\" , # form 1: single string ) t2 = PythonTask ( function = \"os.getcwd\" , name = \"task 2\" , output_extraction = { \"curdir\" : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , \"<% $.outputs.parentdir_content %>\" , \"Current working dir:\" , \"<% $.outputs.curdir %>\" , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"Tutorial 4"},{"location":"tutorials/example04_outputs/#pytailor-example-4","text":"This example introduces the following NEW concepts: For PythonTask definitions: Specifying what to do with function output Accessing function output in downstream tasks Specifying multiple parents The output argument to PythonTask can be specified in two forms: 1. A single string. Then the entire return value of the function is put on $.outputs. . 2. A dictionary. Then for each (tag: query) in the dict the query is applied to the return value and the result is put on $.outputs. The output can be accessed in downstream tasks using query expressions like \"<% $.outputs. %>\". The output is also available as an attribute (dict) on the workflow run objects retrieved from the database (WorkflowRun.outputs). NOTE: Currently this mechanism only work with data that is directly -serializable. In the future non-JSON compatible objects will be handled as well (by use of pickling). from pytailor import PythonTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 1\" , args = [ \"../*.py\" ], output_to = \"parentdir_content\" , # form 1: single string ) t2 = PythonTask ( function = \"os.getcwd\" , name = \"task 2\" , output_extraction = { \"curdir\" : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , \"<% $.outputs.parentdir_content %>\" , \"Current working dir:\" , \"<% $.outputs.curdir %>\" , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"pyTailor Example 4"},{"location":"tutorials/example04_outputs_parameterization/","text":"pyTailor Example 4 from pytailor import PythonTask , DAG , Workflow , Project , Outputs import glob import os ### workflow definition ### outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"../*.py\" ], output_to = outputs . parentdir_content , # form 1: single string ) t2 = PythonTask ( function = os . getcwd , name = \"task 2\" , output_extraction = { outputs . curdir : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , outputs . parentdir_content , \"Current working dir:\" , outputs . curdir , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"Example04 outputs parameterization"},{"location":"tutorials/example04_outputs_parameterization/#pytailor-example-4","text":"from pytailor import PythonTask , DAG , Workflow , Project , Outputs import glob import os ### workflow definition ### outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"../*.py\" ], output_to = outputs . parentdir_content , # form 1: single string ) t2 = PythonTask ( function = os . getcwd , name = \"task 2\" , output_extraction = { outputs . curdir : \"<% $ %>\" }, # form 2: (tag: query) dict ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Python files in parent dir (as list):\" , outputs . parentdir_content , \"Current working dir:\" , outputs . curdir , ], kwargs = { \"sep\" : \" \\n\\n \" , \"end\" : \" \\n\\n \" }, parents = [ t1 , t2 ], ) ### run workflow ### # open a project prj = Project . from_name ( \"Test\" ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"outputs workflow\" ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # outputs are available on the run object print ( \"Outputs are:\" ) print ( wf . outputs )","title":"pyTailor Example 4"},{"location":"tutorials/example05_files/","text":"pyTailor Example 5 This example introduces the following NEW concepts: For task definitions: Specifying files to download before running the task Specifying files to upload after task is run Specify which local files to upload when a workflow is created The download argument to task can be a single file tag (str) or a list of file tags. These file tags refer to files in the fileset associated with the workflow. To send input files into a workflow the following steps are taken: 1. Instantiate a FileSet object 2. Upload files to the fileset with an associated tag 3. Pass the fileset along when instantiating the workflow 4. Tasks will now download files by referencing the file tags. The upload argument to Task is used to specify files to send back to the fileset after a task has been run. upload must be a dict of (tag: val), where val can be: 1. one or more query expressions(str and list of str) which is applied to the function output. The query result is then searched for actual files, these files are then uploaded to storage under the given tag. one or more glob-style strings (str and list of str) which is applied in the task's working dir. Matching files are uploaded under the given tag. File names can be accessed with queries: \"<% $.files. %>\" which is useful when e.g file name(s) are input to functions. from pytailor import PythonTask , DAG , Workflow , Project , FileSet ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 1\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , # refers to a file tag output_to = \"downloaded_files\" , # put function's return value on $.outputs.downloaded_files ) t2 = PythonTask ( function = \"shutil.copyfile\" , name = \"task 2\" , args = [ \"<% $.files.inpfile[0] %>\" , \"newfile.txt\" ], # inpfile is a tag download = \"inpfile\" , upload = { \"outfile\" : \"newfile.txt\" }, ) t3 = PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"Downloaded\" , \"<% $.files.outfile %>\" ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"Tutorial 5"},{"location":"tutorials/example05_files/#pytailor-example-5","text":"This example introduces the following NEW concepts: For task definitions: Specifying files to download before running the task Specifying files to upload after task is run Specify which local files to upload when a workflow is created The download argument to task can be a single file tag (str) or a list of file tags. These file tags refer to files in the fileset associated with the workflow. To send input files into a workflow the following steps are taken: 1. Instantiate a FileSet object 2. Upload files to the fileset with an associated tag 3. Pass the fileset along when instantiating the workflow 4. Tasks will now download files by referencing the file tags. The upload argument to Task is used to specify files to send back to the fileset after a task has been run. upload must be a dict of (tag: val), where val can be: 1. one or more query expressions(str and list of str) which is applied to the function output. The query result is then searched for actual files, these files are then uploaded to storage under the given tag. one or more glob-style strings (str and list of str) which is applied in the task's working dir. Matching files are uploaded under the given tag. File names can be accessed with queries: \"<% $.files. %>\" which is useful when e.g file name(s) are input to functions. from pytailor import PythonTask , DAG , Workflow , Project , FileSet ### workflow definition ### with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 1\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , # refers to a file tag output_to = \"downloaded_files\" , # put function's return value on $.outputs.downloaded_files ) t2 = PythonTask ( function = \"shutil.copyfile\" , name = \"task 2\" , args = [ \"<% $.files.inpfile[0] %>\" , \"newfile.txt\" ], # inpfile is a tag download = \"inpfile\" , upload = { \"outfile\" : \"newfile.txt\" }, ) t3 = PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"Downloaded\" , \"<% $.files.outfile %>\" ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"pyTailor Example 5"},{"location":"tutorials/example05_files_parameterization/","text":"pyTailor Example 5 from pytailor import PythonTask , DAG , Workflow , Project , FileSet , Outputs , Files import glob import shutil ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"*.txt\" ], download = files . testfiles , output_to = outputs . downloaded_files , use_storage_dirs = False ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Downloaded\" , files . outfile ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"Example05 files parameterization"},{"location":"tutorials/example05_files_parameterization/#pytailor-example-5","text":"from pytailor import PythonTask , DAG , Workflow , Project , FileSet , Outputs , Files import glob import shutil ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"*.txt\" ], download = files . testfiles , output_to = outputs . downloaded_files , use_storage_dirs = False ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"Downloaded\" , files . outfile ], download = \"outfile\" , parents = t2 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"files workflow\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state ) # print the output of task 1 print ( \"Downloaded files: \\n \" , wf . outputs . get ( \"downloaded_files\" ))","title":"pyTailor Example 5"},{"location":"tutorials/example06_branch_task/","text":"pyTailor Example 6 This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching nch_data* is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ \"<% $.inputs.data %>\" ]): PythonTask ( function = \"builtins.print\" , name = \"task 1\" , args = [ \"<% $.inputs.data %>\" , \"<% $.inputs.other %>\" ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Tutorial 6"},{"location":"tutorials/example06_branch_task/#pytailor-example-6","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching nch_data* is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project ### workflow definition ### with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ \"<% $.inputs.data %>\" ]): PythonTask ( function = \"builtins.print\" , name = \"task 1\" , args = [ \"<% $.inputs.data %>\" , \"<% $.inputs.other %>\" ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Example 6"},{"location":"tutorials/example06_branch_task_parameterization/","text":"pyTailor Example 6 This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching nch_data* is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , Inputs ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ inputs . data ]): PythonTask ( function = print , name = \"task 1\" , args = [ inputs . data , inputs . other ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) wf_inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Example06 branch task parameterization"},{"location":"tutorials/example06_branch_task_parameterization/#pytailor-example-6","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a single PythonTask For BranchTask definitions: Use branch_data to specify which data to use for branching nch_data* is given as one or more query-expressions. When branching is performed query-expressions must evaluate to to a list or a dict. If the queries evaluates to a dict, that dict must have integer keys to represent the index of each branch. Branched tasks always become children of the BranchTask that created them. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , Inputs ### workflow definition ### inputs = Inputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"duplicate\" , branch_data = [ inputs . data ]): PythonTask ( function = print , name = \"task 1\" , args = [ inputs . data , inputs . other ], ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) wf_inputs = { \"data\" : [ 1 , 2 ], # 'data': {0: 1, 1: 2}, # alternatively use a dict with int keys \"other\" : \"this is not used for branching\" , } # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow\" , inputs = wf_inputs ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Example 6"},{"location":"tutorials/example07_branch_dag/","text":"pyTailor Example 7 This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , FileSet ### workflow definition ### with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) inputs = {} # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , inputs = inputs , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Tutorial 7"},{"location":"tutorials/example07_branch_dag/#pytailor-example-7","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import PythonTask , BranchTask , DAG , Workflow , Project , FileSet ### workflow definition ### with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ \"<% $.files.testfiles %>\" ], branch_files = [ \"testfiles\" ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = \"testfiles\" , output_to = \"glob_res\" , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ \"<% $.files.testfiles %>\" , \"<% $.outputs.glob_res %>\" ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) inputs = {} # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , inputs = inputs , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Example 7"},{"location":"tutorials/example07_branch_dag_parameterization/","text":"pyTailor Example 7 This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import ( PythonTask , BranchTask , DAG , Workflow , Project , FileSet , Files , Outputs , ) ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ files . testfiles ], branch_files = [ files . testfiles ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , output_to = outputs . glob_res , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ files . testfiles , outputs . glob_res ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"Example07 branch dag parameterization"},{"location":"tutorials/example07_branch_dag_parameterization/#pytailor-example-7","text":"This example introduces the following NEW concepts: Use BranchTask to \"branch out\" a DAG For BranchTask definitions: Use branch_files to specify which files to use for branching branch_files is given as one or more file tags. from pytailor import ( PythonTask , BranchTask , DAG , Workflow , Project , FileSet , Files , Outputs , ) ### workflow definition ### files = Files () outputs = Outputs () with DAG ( name = \"dag\" ) as dag : with BranchTask ( name = \"branch\" , branch_data = [ files . testfiles ], branch_files = [ files . testfiles ], ): with DAG ( name = \"sub-dag\" ) as sub_dag : t1 = PythonTask ( function = \"glob.glob\" , name = \"task 2\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , output_to = outputs . glob_res , ) PythonTask ( function = \"builtins.print\" , name = \"task 3\" , args = [ files . testfiles , outputs . glob_res ], parents = t1 , ) ### workflow run ### # open a project prj = Project . from_name ( \"Test\" ) # create a fileset and upload files fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ]) # create a workflow: wf = Workflow ( project = prj , dag = dag , name = \"branch workflow 2\" , fileset = fileset ) # run the workflow wf . run () # check the status of the workflow print ( \"The workflow finished with state:\" ) print ( wf . state )","title":"pyTailor Example 7"},{"location":"tutorials/example08_add_workflow_definition/","text":"pyTailor Example 8 This example introduces the following NEW concepts: Create WorkflowDefinition For WorkflowDefinition: - use *inputsschema* to specify the allowed parameters in main DAG - use *filesschema* to specify the allowed files in main DAG - use *description* to describe the workflow definition Use Account to manage your workflow definitions Add the workflow definition to a specific project. from pytailor import PythonTask , DAG , WorkflowDefinition , Account , Project , \\ InputsSchema , FilesSchema , Description , Files , Outputs , Inputs , Workflow , FileSet import glob import shutil # a modified dag description with parametrization from example 5 files = Files () outputs = Outputs () inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , # refers to a file tag output_to = outputs . downloaded_files , # put function's return value on # outputs.downloaded_files ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"My input print\" , inputs . print ], parents = t1 , ) # define inputsschema example_inputs = { 'print' : 'hello, world!' } inputs_schema = InputsSchema ( inputs = example_inputs ) inputs_schema . add_defaults ( example_inputs ) # define filesschema files_schema = FilesSchema () files_schema . add_file ( tag = 'inpfile' , ext = [ 'txt' ], required = True , multiple = False ) files_schema . add_file ( tag = 'testfiles' , ext = [ 'txt' ], required = True , multiple = True ) wf_def_description = \"\"\" This workflow definition has the following steps: - Download testfiles and sends filename as output - Download inpfile and upload files - prints the inputs.print argument \"\"\" description = Description . from_dag ( dag , wf_def_name = 'Example 8 Hello world workflow ' 'definition' , wf_def_description = wf_def_description ) # create the workflow definition wf_def = WorkflowDefinition ( name = description . name , description = description . to_string (), dag = dag , inputs_schema = inputs_schema . to_dict (), files_schema = files_schema . to_dict () ) # get an account and add wf_def to account # (requires account admin privileges) account = Account . get_my_accounts ()[ 0 ] wf_def . add_to_account ( account ) # wf_def has now gotten an id print ( wf_def . id ) # the workflow definition can now be added to a project # (requires account admin privileges) prj = Project . from_name ( \"Test\" ) prj . add_workflow_definition ( wf_def . id ) # if you want ... prj . list_available_workflow_definitions () wf_def = WorkflowDefinition . from_project_and_id ( prj , wf_def . id ) fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) wf = Workflow ( project = prj , dag = wf_def . dag , name = \"my workflow\" , inputs = example_inputs , fileset = fileset ) # not yet implemented: # Workflow.from_definition(wf_def.id, # name=\"my workflow\", # inputs=example_inputs, # fileset=fileset) wf . run () # if you want ... prj . remove_workflow_definition ( wf_def . id )","title":"Tutorial 8"},{"location":"tutorials/example08_add_workflow_definition/#pytailor-example-8","text":"This example introduces the following NEW concepts: Create WorkflowDefinition For WorkflowDefinition: - use *inputsschema* to specify the allowed parameters in main DAG - use *filesschema* to specify the allowed files in main DAG - use *description* to describe the workflow definition Use Account to manage your workflow definitions Add the workflow definition to a specific project. from pytailor import PythonTask , DAG , WorkflowDefinition , Account , Project , \\ InputsSchema , FilesSchema , Description , Files , Outputs , Inputs , Workflow , FileSet import glob import shutil # a modified dag description with parametrization from example 5 files = Files () outputs = Outputs () inputs = Inputs () with DAG ( name = \"dag\" ) as dag : t1 = PythonTask ( function = glob . glob , name = \"task 1\" , args = [ \"**/*.txt\" ], kwargs = { \"recursive\" : True }, download = files . testfiles , # refers to a file tag output_to = outputs . downloaded_files , # put function's return value on # outputs.downloaded_files ) t2 = PythonTask ( function = shutil . copyfile , name = \"task 2\" , args = [ files . inpfile [ 0 ], \"newfile.txt\" ], download = files . inpfile , upload = { files . outfile : \"newfile.txt\" }, ) t3 = PythonTask ( function = print , name = \"task 3\" , args = [ \"My input print\" , inputs . print ], parents = t1 , ) # define inputsschema example_inputs = { 'print' : 'hello, world!' } inputs_schema = InputsSchema ( inputs = example_inputs ) inputs_schema . add_defaults ( example_inputs ) # define filesschema files_schema = FilesSchema () files_schema . add_file ( tag = 'inpfile' , ext = [ 'txt' ], required = True , multiple = False ) files_schema . add_file ( tag = 'testfiles' , ext = [ 'txt' ], required = True , multiple = True ) wf_def_description = \"\"\" This workflow definition has the following steps: - Download testfiles and sends filename as output - Download inpfile and upload files - prints the inputs.print argument \"\"\" description = Description . from_dag ( dag , wf_def_name = 'Example 8 Hello world workflow ' 'definition' , wf_def_description = wf_def_description ) # create the workflow definition wf_def = WorkflowDefinition ( name = description . name , description = description . to_string (), dag = dag , inputs_schema = inputs_schema . to_dict (), files_schema = files_schema . to_dict () ) # get an account and add wf_def to account # (requires account admin privileges) account = Account . get_my_accounts ()[ 0 ] wf_def . add_to_account ( account ) # wf_def has now gotten an id print ( wf_def . id ) # the workflow definition can now be added to a project # (requires account admin privileges) prj = Project . from_name ( \"Test\" ) prj . add_workflow_definition ( wf_def . id ) # if you want ... prj . list_available_workflow_definitions () wf_def = WorkflowDefinition . from_project_and_id ( prj , wf_def . id ) fileset = FileSet ( prj ) fileset . upload ( testfiles = [ \"testfiles/testfile_01.txt\" , \"testfiles/testfile_02.txt\" ], inpfile = [ \"testfiles/testfile_03.txt\" ], ) wf = Workflow ( project = prj , dag = wf_def . dag , name = \"my workflow\" , inputs = example_inputs , fileset = fileset ) # not yet implemented: # Workflow.from_definition(wf_def.id, # name=\"my workflow\", # inputs=example_inputs, # fileset=fileset) wf . run () # if you want ... prj . remove_workflow_definition ( wf_def . id )","title":"pyTailor Example 8"}]}